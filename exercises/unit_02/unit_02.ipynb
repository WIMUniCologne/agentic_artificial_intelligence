{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0587917",
   "metadata": {},
   "source": [
    "# Agentic Artificial Intelligence\n",
    "## Exercise - Unit 02: Large Language Models\n",
    "\n",
    "Welcome to the second unit of the Agentic Artificial Intelligence course! \n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this unit, you will:\n",
    "1. understand tokenizers for language\n",
    "2. understand chat templates and prompt templates\n",
    "3. understand how to build simple LLM applications using chains\n",
    "4. build your first (simple) LLM powered AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134ab94",
   "metadata": {},
   "source": [
    "## What are LLMs?\n",
    "Large Language Models (LLMs) are AI models proficient in comprehending and producing human language. Trained on extensive text datasets, they acquire linguistic patterns, structures, and nuances, and are typically characterized by millions of parameters.\n",
    "\n",
    "The majority of contemporary LLMs leverage the Transformer architecture, a deep learning model rooted in the \"Attention\" mechanism, which has garnered considerable attention since Google's introduction of BERT in 2018.\n",
    "\n",
    "LLMs operate on a straightforward, highly effective principle: predicting the subsequent token based on a preceding sequence. A \"token\" is the fundamental unit of information an LLM processes, analogous to a word but optimized for efficiency, often representing sub-word units. For example, an LLM's vocabulary might be significantly smaller than the total number of words in a language (e.g., Llama 2 with ~32,000 tokens), utilizing combinable sub-word tokens like \"interest\" and \"ing\" to form \"interesting,\" or appending \"ed\" for \"interested.\" You can explore various tokenizers in the interactive playground below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07071607",
   "metadata": {},
   "source": [
    "### Experimenting with tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cde9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!'\n",
      "Token IDs: [101, 19204, 3989, 2003, 17160, 1998, 1996, 2607, 4005, 2594, 7976, 4454, 2003, 22371, 999, 102]\n",
      "Tokens: ['[CLS]', 'token', '##ization', 'is', 'fascinating', 'and', 'the', 'course', 'agent', '##ic', 'artificial', 'intelligence', 'is', 'exiting', '!', '[SEP]']\n",
      "Decoded Text: '[CLS] tokenization is fascinating and the course agentic artificial intelligence is exiting! [SEP]'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "text_to_tokenize = \"Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!\"\n",
    "\n",
    "\n",
    "def tokenize_text(text_to_tokenize, tokenizer_name = \"bert-base-uncased\"):\n",
    "\n",
    "    # Load a tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # 1. Encode the text into token IDs\n",
    "    encoded_ids = tokenizer.encode(text_to_tokenize)\n",
    "    print(f\"Original Text: '{text_to_tokenize}'\")\n",
    "    print(f\"Token IDs: {encoded_ids}\")\n",
    "\n",
    "    # 2. Convert the IDs back into tokens (the actual text pieces)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_ids)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "\n",
    "    # 3. Decode the IDs back to the original string\n",
    "    decoded_text = tokenizer.decode(encoded_ids)\n",
    "    print(f\"Decoded Text: '{decoded_text}'\")\n",
    "\n",
    "tokenize_text(text_to_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7c4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!'\n",
      "Token IDs: [30642, 1634, 318, 13899, 290, 262, 1781, 15906, 291, 35941, 9345, 318, 33895, 0]\n",
      "Tokens: ['Token', 'ization', 'ƒ†is', 'ƒ†fascinating', 'ƒ†and', 'ƒ†the', 'ƒ†course', 'ƒ†Agent', 'ic', 'ƒ†Artificial', 'ƒ†Intelligence', 'ƒ†is', 'ƒ†exiting', '!']\n",
      "Decoded Text: 'Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!'\n"
     ]
    }
   ],
   "source": [
    "tokenize_text(text_to_tokenize, 'gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167de6c",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) utilize special, model-specific tokens to structure their generated output and input prompts. These tokens delineate components like sequences, messages, and responses, with the End-of-Sequence (EOS) token being particularly crucial. The exact form and variety of these special tokens differ significantly across different LLM providers, as further illustrated in the table below.\n",
    "\n",
    "| Model | Provider | EOS Token | Functionality |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **GPT4** | OpenAI | `<endoftext>` | End of message text |\n",
    "| **Llama 3** | Meta (Facebook AI Research) | `<\\|eot_id\\|>` | End of sequence |\n",
    "| **Deepseek-R1** | DeepSeek | `<\\|end_of_sentence\\|>` | End of message text |\n",
    "| **SmolLM2** | Hugging Face | `<\\|im_end\\|>` | End of instruction or message |\n",
    "| **Gemma** | Google | `<end_of_turn>` | End of conversation turn |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce07ac",
   "metadata": {},
   "source": [
    "### Next token prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff1e74",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) operate autoregressively, where each generated token serves as input for predicting the subsequent token. This iterative process continues until the model generates an End-of-Sequence (EOS) token, signaling the completion of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0b747",
   "metadata": {},
   "source": [
    "<img src=\"AutoregressionSchema.gif\" alt=\"Alt text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db16fb",
   "metadata": {},
   "source": [
    "An LLM decodes text iteratively until it encounters the End-of-Sequence (EOS) token. During each decoding loop:\n",
    "\n",
    "1.  **Input Processing:** The input text is first tokenized. The model then generates a comprehensive representation of this token sequence, encoding both the meaning and positional information of each token.\n",
    "2.  **Likelihood Scoring:** This sequence representation is fed into the model, which subsequently outputs scores indicating the probability of every token in its vocabulary being the next in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b098f37",
   "metadata": {},
   "source": [
    "<img src=\"DecodingFinal.gif\" alt=\"Alt text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4f93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://agents-course-decoding-visualizer.hf.space ‚úî\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from gradio_client import Client\n",
    "\n",
    "def get_decoding_visualization(input_text):\n",
    "    \"\"\"\n",
    "    Calls the remote Gradio app and returns the HTML visualization.\n",
    "    \"\"\"\n",
    "    client = Client(\"agents-course/decoding_visualizer\")\n",
    "    result = client.predict(\n",
    "        input_text=input_text,\n",
    "        api_name=\"/get_beam_search_html\"\n",
    "    )\n",
    "    \n",
    "    # The client.predict() result may be a tuple; handle it to get the string.\n",
    "    if isinstance(result, tuple):\n",
    "        html_string = result[0]\n",
    "    else:\n",
    "        html_string = result\n",
    "        \n",
    "    return gr.HTML(value=html_string)\n",
    "\n",
    "# Create the Gradio Interface\n",
    "demo = gr.Interface(\n",
    "    fn=get_decoding_visualization,\n",
    "    inputs=gr.Textbox(label=\"Input Text\", value=\"The Capital of France is\"),\n",
    "    outputs=gr.HTML(label=\"Decoding Visualization\"),\n",
    "    title=\"Decoding Visualizer\",\n",
    "    description=\"Visualize decoding steps from a remote model by entering text below.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f9173",
   "metadata": {},
   "source": [
    "If you want to learn more about Natural Language Processing, I recommend you to check out the Hugginfaces NLP course: https://huggingface.co/learn/llm-course/chapter1/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2dc461",
   "metadata": {},
   "source": [
    "## Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7b4c8",
   "metadata": {},
   "source": [
    "In the Transformer architecture, a crucial element is \"Attention.\" This mechanism recognizes that when predicting the next word, not all words in a sentence contribute equally to the meaning. For instance, in the sentence \"The capital of France is ‚Ä¶\", words such as \"France\" and \"capital\" are most significant for determining the subsequent word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71926516",
   "metadata": {},
   "source": [
    "<img src=\"AttentionSceneFinal.gif\" alt=\"Alt text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f2aff",
   "metadata": {},
   "source": [
    "The ability to pinpoint the most relevant words for predicting the next token has significantly boosted LLM effectiveness. While the fundamental principle of next-token prediction remains, substantial progress since GPT-2 has focused on scaling neural networks and extending the attention mechanism to handle increasingly longer sequences. This has led to the concept of \"context length,\" which defines the maximum number of tokens an LLM can process and its corresponding attention span."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058190ec",
   "metadata": {},
   "source": [
    "## How can I use LLMs?\n",
    "\n",
    "You have two primary methods for utilizing models:\n",
    "\n",
    "1.  **Local Execution:** This option is viable if your hardware meets the necessary specifications.\n",
    "2.  **Cloud/API Access:** You can leverage cloud services, such as the Hugging Face Serverless Inference API.\n",
    "\n",
    "In this course, we will primarily interact with models through APIs, with a later focus on deploying and running these models on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079c45c",
   "metadata": {},
   "source": [
    "## Making API Calls to LLMs with LangChain and Gemini\n",
    "\n",
    "Now that we understand how LLMs work internally, let's learn how to actually use them in practice through API calls. We'll use **LangChain**, a popular framework for building applications with LLMs, and **Google's Gemini** as our LLM provider.\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "LangChain is a framework designed to simplify the development of applications using large language models. It provides:\n",
    "\n",
    "- **Unified API**: Work with different LLM providers (OpenAI, Google, Anthropic, etc.) using a consistent interface\n",
    "- **Prompt Templates**: Structure and manage prompts effectively\n",
    "- **Memory**: Maintain conversation context across multiple interactions\n",
    "- **Chains**: Combine multiple LLM calls and operations\n",
    "- **Tools**: Integrate LLMs with external APIs and services\n",
    "\n",
    "### Why Use APIs Instead of Local Models?\n",
    "\n",
    "1. **No Hardware Requirements**: Don't need powerful GPUs\n",
    "2. **Latest Models**: Access to state-of-the-art models like Gemini Pro\n",
    "3. **Scalability**: Handle multiple requests without resource constraints\n",
    "4. **Maintenance-Free**: No need to manage model updates or infrastructure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5abd67",
   "metadata": {},
   "source": [
    "### Setting Up Your Environment\n",
    "\n",
    "Before we can make API calls, we need to set up our environment with the necessary API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18b8191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google API key found!\n",
      "Key ends with:****eg\n"
     ]
    }
   ],
   "source": [
    "# First, let's install the required packages (if not already installed)\n",
    "from agentic_ai.utils.helpers import check_api_setup\n",
    "\n",
    "# Check if we have the required API key\n",
    "api_configured = check_api_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46c0e3",
   "metadata": {},
   "source": [
    "### Your First LLM API Call\n",
    "\n",
    "Let's make our first API call to Google's Gemini model using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e0cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Imagine a **super-smart, incredibly well-read robot that can understand and generate human-like text.** That's essentially what a **Large Language Model (LLM)** is.\n",
      "\n",
      "Here's a breakdown in simple terms:\n",
      "\n",
      "*   **\"Large\":** This means it has been trained on an enormous amount of text data ‚Äì think of it like reading a giant library filled with books, articles, websites, and conversations from all over the internet. This massive exposure allows it to learn patterns, grammar, facts, and different writing styles.\n",
      "\n",
      "*   **\"Language\":** Its primary job is to work with human language, both understanding what you say and creating new text.\n",
      "\n",
      "*   **\"Model\":** In computer science, a \"model\" is like a set of rules and knowledge that a computer program uses to perform a task. So, an LLM is a model specifically designed for language.\n",
      "\n",
      "**What can it do?**\n",
      "\n",
      "Because it's so well-read and understands language so deeply, an LLM can do many things, such as:\n",
      "\n",
      "*   **Answer questions:** You can ask it almost anything, and it will try its best to give you a helpful answer based on what it has learned.\n",
      "*   **Write stories, poems, or emails:** You can give it a prompt, and it can generate creative text in various styles.\n",
      "*   **Translate languages:** It can understand one language and translate it into another.\n",
      "*   **Summarize long texts:** If you have a lengthy article, it can condense it into a shorter, easier-to-understand summary.\n",
      "*   **Explain complex topics:** It can break down difficult ideas into simpler terms.\n",
      "*   **Have conversations:** It can chat with you in a way that feels natural.\n",
      "\n",
      "**How does it work (very simply)?**\n",
      "\n",
      "Think of it like this: When you read, you learn how words go together and what they mean. An LLM does something similar, but on a massive scale. It learns the probability of certain words appearing together. For example, after reading millions of sentences, it knows that \"the cat sat on the...\" is very likely to be followed by \"mat\" or \"rug,\" rather than \"rocket\" or \"mountain.\"\n",
      "\n",
      "When you give it a prompt, it uses this learned knowledge to predict the most likely next word, and then the next word, and so on, to create a coherent and relevant response.\n",
      "\n",
      "**In short:** An LLM is a powerful computer program that has learned so much about language that it can understand what you're saying and generate text that sounds like a human wrote it.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = init_chat_model(\"gemini-2.5-flash-lite\", model_provider=\"google_genai\", temperature=0.7, max_tokens=2000)\n",
    "\n",
    "# Make your first API call!\n",
    "response = llm.invoke(\"Explain what a Large Language Model is in simple terms.\")\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ff858",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "The LLM doesn't just return a string - it returns a rich response object with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e31144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', 'Imagine a **super-smart, incredibly well-read robot that can understand and generate human-like text.** That\\'s essentially what a **Large Language Model (LLM)** is.\\n\\nHere\\'s a breakdown in simple terms:\\n\\n*   **\"Large\":** This means it has been trained on an enormous amount of text data ‚Äì think of it like reading a giant library filled with books, articles, websites, and conversations from all over the internet. This massive exposure allows it to learn patterns, grammar, facts, and different writing styles.\\n\\n*   **\"Language\":** Its primary job is to work with human language, both understanding what you say and creating new text.\\n\\n*   **\"Model\":** In computer science, a \"model\" is like a set of rules and knowledge that a computer program uses to perform a task. So, an LLM is a model specifically designed for language.\\n\\n**What can it do?**\\n\\nBecause it\\'s so well-read and understands language so deeply, an LLM can do many things, such as:\\n\\n*   **Answer questions:** You can ask it almost anything, and it will try its best to give you a helpful answer based on what it has learned.\\n*   **Write stories, poems, or emails:** You can give it a prompt, and it can generate creative text in various styles.\\n*   **Translate languages:** It can understand one language and translate it into another.\\n*   **Summarize long texts:** If you have a lengthy article, it can condense it into a shorter, easier-to-understand summary.\\n*   **Explain complex topics:** It can break down difficult ideas into simpler terms.\\n*   **Have conversations:** It can chat with you in a way that feels natural.\\n\\n**How does it work (very simply)?**\\n\\nThink of it like this: When you read, you learn how words go together and what they mean. An LLM does something similar, but on a massive scale. It learns the probability of certain words appearing together. For example, after reading millions of sentences, it knows that \"the cat sat on the...\" is very likely to be followed by \"mat\" or \"rug,\" rather than \"rocket\" or \"mountain.\"\\n\\nWhen you give it a prompt, it uses this learned knowledge to predict the most likely next word, and then the next word, and so on, to create a coherent and relevant response.\\n\\n**In short:** An LLM is a powerful computer program that has learned so much about language that it can understand what you\\'re saying and generate text that sounds like a human wrote it.')\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run--869b0c03-2867-473a-9503-fcbb1587e542-0')\n",
      "('example', False)\n",
      "('tool_calls', [])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 12, 'output_tokens': 546, 'total_tokens': 558, 'input_token_details': {'cache_read': 0}})\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad85dccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Response Object Details:\n",
      "========================================\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: Imagine a **super-smart, incredibly well-read robot that can understand and generate human-like text...\n",
      "Response metadata: {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}\n",
      "Usage metadata: {'input_tokens': 12, 'output_tokens': 546, 'total_tokens': 558, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"üìã Response Object Details:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Type: {type(response)}\")\n",
    "print(f\"Content: {response.content[:100]}...\")\n",
    "print(f\"Response metadata: {response.response_metadata}\")\n",
    "print(f\"Usage metadata: {response.usage_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e1035",
   "metadata": {},
   "source": [
    "üîç Available Attributes:\n",
    "- content: The actual text response\n",
    "- response_metadata: Model-specific metadata\n",
    "- usage_metadata: Token usage information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e81109",
   "metadata": {},
   "source": [
    "### Understanding with Chat Templates\n",
    "\n",
    "Recap: A chat template's job is to convert a list of messages into a single, formatted string.\n",
    "\n",
    "Let's define a sample conversation as a list of dictionaries. This is a standard format you'll encounter frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe0fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Gemini, a helpful AI assistant built by Google.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you write a short, 3-line poem about programming?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure, here's a poem about programming:...\"},\n",
    "    {\"role\": \"user\", \"content\": \"That's great! Can you explain chat templates in LLMs?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a733e5b",
   "metadata": {},
   "source": [
    "If we were to guess the format, we might just join the content together. However, this would be the wrong approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f34db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are Gemini, a helpful AI assistant built by Google.\n",
      "user: Hello! Can you write a short, 3-line poem about programming?\n",
      "assistant: Sure, here's a poem about programming:...\n",
      "user: That's great! Can you explain chat templates in LLMs?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is NOT the correct way to do it!\n",
    "manual_prompt = \"\"\n",
    "for message in conversation:\n",
    "    manual_prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "print(manual_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fc11f",
   "metadata": {},
   "source": [
    "Feeding this string to a model does not result in the optimal response because it's not the format the model was trained on.<br>\n",
    "I won't demonstrate it here as langchain applies the respective chat template for us automatically.\n",
    "\n",
    "Instead, we should use a chat template to format the conversation.\n",
    "\n",
    "The good thing about using langchain is that it handles this for us automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf69f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are Gemini, a helpful AI assistant built by Google.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello! Can you write a short, 3-line poem about programming?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Sure, here's a poem about programming:...\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"That's great! Can you explain chat templates in LLMs?\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# showing how langchain applies the respective chat template for us automatically\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(conversation)\n",
    "print(prompt.invoke({\"role\": \"user\", \"content\": \"Hello! Can you write a short, 3-line poem about programming?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ba713",
   "metadata": {},
   "source": [
    "- As you can see, langchain automatically interpreted our conversation json and put it into the correct format.\n",
    "- It does this by using its own classes SystemMessage, HumanMessage, AIMessage which are then applied to the chat template of a specific model.\n",
    "- Note: This only works with models supported by the langchain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf26b904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a helpful assistant that can answer questions and help with tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Germany?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that can answer questions and help with tasks.\")\n",
    "human_message_1 = HumanMessage(content=\"What is the capital of France?\")\n",
    "ai_message = AIMessage(content=\"The capital of France is Paris.\")\n",
    "human_message_2 = HumanMessage(content=\"What is the capital of Germany?\")\n",
    "\n",
    "# Now let's put this into a chat template\n",
    "prompt = ChatPromptTemplate.from_messages([system_message, human_message_1, ai_message, human_message_2])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fb2277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that can answer questions and help with tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Germany?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First invoke the prompt template to get formatted messages, then pass to LLM\n",
    "prompt.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4590dd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--7b05ed1a-7197-4d6a-ad1d-2f03650eacc3-0', usage_metadata={'input_tokens': 38, 'output_tokens': 7, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the prompt template to get formatted messages, then pass to LLM\n",
    "llm.invoke(prompt.invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e536c21",
   "metadata": {},
   "source": [
    "### Working with Prompt Templates\n",
    "\n",
    "- Not to be confused with chat templates!\n",
    "- Instead of hardcoding prompts, we can use templates to make our prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9956fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated Prompt:\n",
      "========================================\n",
      "You are an expert machine learning engineer. \n",
      "\n",
      "Question: How do transformer models work?\n",
      "\n",
      "Please provide a detailed answer that includes:\n",
      "1. A clear explanation\n",
      "2. Real-world examples\n",
      "3. Practical implications\n",
      "\n",
      "Answer:\n",
      "\n",
      "========================================\n",
      "\n",
      "ü§ñ Response:\n",
      "As an expert machine learning engineer, I'm thrilled to delve into the fascinating world of Transformer models. They have revolutionized Natural Language Processing (NLP) and are increasingly making their mark in other domains. Let's break down how they work in detail.\n",
      "\n",
      "## How Transformer Models Work: A Deep Dive\n",
      "\n",
      "At their core, Transformer models are a type of neural network architecture designed to handle **sequential data**, particularly text, with remarkable efficiency and effectiveness. Before Transformers, Recurrent Neural Networks (RNNs) like LSTMs and GRUs were the dominant force in NLP. However, RNNs process data one element at a time, leading to two main limitations:\n",
      "\n",
      "1.  **Sequential Bottleneck:** This makes parallelization difficult, slowing down training on large datasets.\n",
      "2.  **Vanishing/Exploding Gradients:** Information from early parts of a long sequence can be lost or become distorted by the time it reaches the end.\n",
      "\n",
      "Transformers overcome these limitations by entirely abandoning recurrence and instead relying on a mechanism called **attention**.\n",
      "\n",
      "### 1. The Core Idea: Attention is All You Need\n",
      "\n",
      "The seminal paper that introduced Transformers, aptly titled \"Attention Is All You Need,\" highlighted the power of attention mechanisms. Instead of processing sequences sequentially, Transformers allow the model to **weigh the importance of different parts of the input sequence when processing a specific element.** This is analogous to how humans read: when understanding a sentence, we don't just focus on the current word; we recall and consider relevant words from earlier in the sentence to grasp the full meaning.\n",
      "\n",
      "#### The Transformer Architecture: Encoder-Decoder Structure\n",
      "\n",
      "The original Transformer model follows an **encoder-decoder architecture**, commonly used for sequence-to-sequence tasks like machine translation.\n",
      "\n",
      "**A. The Encoder:**\n",
      "The encoder's job is to process the input sequence (e.g., a sentence in English) and convert it into a rich, contextualized representation. It consists of a stack of identical layers, each containing two sub-layers:\n",
      "\n",
      "1.  **Multi-Head Self-Attention Mechanism:** This is the heart of the Transformer.\n",
      "2.  **Position-wise Feed-Forward Network:** A simple, fully connected feed-forward network applied independently to each position.\n",
      "\n",
      "**How Self-Attention Works (The Magic):**\n",
      "\n",
      "Self-attention allows each word in the input sequence to \"look at\" every other word in the same sequence and determine how relevant they are to its own meaning. It does this by calculating **attention scores**. For each word, we generate three vectors:\n",
      "\n",
      "*   **Query (Q):** Represents what information the current word is \"looking for.\"\n",
      "*   **Key (K):** Represents what information each other word \"offers.\"\n",
      "*   **Value (V):** Represents the actual content of each other word.\n",
      "\n",
      "The process for a single attention head is as follows:\n",
      "\n",
      "*   **Step 1: Calculate Similarity Scores:** For each word, its Query vector is multiplied with the Key vectors of *all* other words (including itself). This produces a similarity score, indicating how well each other word's \"offering\" matches the current word's \"request.\"\n",
      "    *   `Scores = Q * K^T` (where K^T is the transpose of K)\n",
      "*   **Step 2: Scale and Softmax:** The scores are scaled down (to prevent exploding gradients) and then passed through a softmax function. This converts the scores into probabilities, ensuring they sum up to 1. These probabilities represent the **attention weights**.\n",
      "    *   `Attention Weights = softmax(Scores / sqrt(d_k))` (where `d_k` is the dimension of the key vectors)\n",
      "*   **Step 3: Weighted Sum of Values:** Each word's Value vector is multiplied by its corresponding attention weight. These weighted Value vectors are then summed up. The result is a new vector representation for the current word that incorporates information from all other words, weighted by their relevance.\n",
      "    *   `Output = Attention Weights * V`\n",
      "\n",
      "**Multi-Head Attention:** Instead of performing self-attention once, Transformers use **multi-head attention**. This means they have multiple sets of Q, K, and V matrices, allowing the model to attend to different aspects of the relationships between words simultaneously. For example, one head might focus on grammatical relationships, while another focuses on semantic relationships. The outputs from all these \"heads\" are concatenated and linearly transformed to produce the final output of the multi-head attention sub-layer.\n",
      "\n",
      "**Positional Encoding:** Since Transformers don't have recurrence, they lose the inherent notion of word order. To address this, **positional encodings** are added to the input embeddings. These are vectors that represent the position of each word in the sequence, allowing the model to distinguish between words that are the same but appear in different positions.\n",
      "\n",
      "**B. The Decoder:**\n",
      "The decoder's job is to generate the output sequence (e.g., a sentence in French) based on the encoder's output and the previously generated output tokens. It also consists of a stack of identical layers, but with an additional sub-layer:\n",
      "\n",
      "1.  **Masked Multi-Head Self-Attention:** Similar to the encoder's self-attention, but \"masked\" to prevent attending to future tokens in the *output* sequence. This ensures that when predicting a word, the model only uses information from words it has already generated.\n",
      "2.  **Multi-Head Cross-Attention:** This mechanism allows the decoder to attend to the output of the encoder. It uses the decoder's current state as the Query and the encoder's output as the Keys and Values. This helps the decoder focus on the most relevant parts of the input sequence for generating the current output word.\n",
      "3.  **Position-wise Feed-Forward Network:** Same as in the encoder.\n",
      "\n",
      "**Output Layer:** Finally, the decoder's output is passed through a linear layer and a softmax function to predict the probability distribution over the vocabulary for the next word in the output sequence.\n",
      "\n",
      "### 2. Real-World Examples\n",
      "\n",
      "The impact of Transformers is undeniable. Here are some prominent examples:\n",
      "\n",
      "*   **Machine Translation (e.g., Google Translate):** This was the initial driving force behind the Transformer. Models like Google's **Google Neural Machine Translation (GNMT)**, which later adopted Transformer-like architectures, significantly improved translation quality by understanding sentence context better.\n",
      "*   **Text Generation (e.g., ChatGPT, Bard, GPT-3/4):** Large Language Models (LLMs) like OpenAI's GPT series and Google's LaMDA/PaLM are built upon the Transformer architecture. They excel at generating human-like text for various purposes, including:\n",
      "    *   **Writing articles, stories, poems, and code.**\n",
      "    *   **Answering questions and providing explanations.**\n",
      "    *   **Summarizing long texts.**\n",
      "    *   **Engaging in conversational dialogue.**\n",
      "*   **Text Summarization (e.g., Hugging Face Transformers library models):** Transformers are used to condense lengthy documents into shorter, coherent summaries.\n",
      "*   **Question Answering (e.g., BERT, RoBERTa):** Models like BERT (Bidirectional Encoder Representations from Transformers) are pre-trained on massive text datasets and then fine-tuned for specific tasks. They can understand the context of a passage and accurately answer questions about it.\n",
      "*   **Sentiment Analysis:** Transformers can analyze text to determine the emotional tone (positive, negative, neutral).\n",
      "*   **Image Captioning:** While primarily for text, the attention mechanism has been adapted for vision tasks. Models can process an image and generate a descriptive caption.\n",
      "*   **Code Generation (e.g., GitHub Copilot):** LLMs based on Transformers can suggest code snippets, complete functions, and even generate entire programs based on natural language prompts.\n",
      "\n",
      "### 3. Practical Implications\n",
      "\n",
      "The advent of Transformers has profound practical implications across various fields:\n",
      "\n",
      "*   **Democratization of AI:** Pre-trained Transformer models (like those available on Hugging Face) allow developers and researchers with less computational power to leverage state-of-the-art NLP capabilities without training massive models from scratch. This significantly lowers the barrier to entry for AI development.\n",
      "*   **Enhanced User Experiences:**\n",
      "    *   **More natural and helpful chatbots and virtual assistants.**\n",
      "    *   **Improved search engines that understand intent rather than just keywords.**\n",
      "    *   **Personalized content recommendations.**\n",
      "    *   **More efficient customer support through automated responses.**\n",
      "*   **Increased Productivity:**\n",
      "    *   **Automated content creation for marketing, journalism, and creative writing.**\n",
      "    *   **Faster code development and debugging with AI assistants.**\n",
      "    *   **Streamlined information retrieval and analysis.**\n",
      "*   **New Research Frontiers:** Transformers have opened up new avenues for research in areas like:\n",
      "    *   **Multimodal learning (combining text, images, audio).**\n",
      "    *   **Low-resource language processing.**\n",
      "    *   **Explainable AI (understanding why a model makes certain decisions).**\n",
      "*   **Ethical Considerations and Challenges:**\n",
      "    *   **Bias amplification:** Transformers trained on biased data can perpetuate and amplify those biases.\n",
      "    *   **Misinformation and deepfakes:** The ability to generate realistic text can be misused.\n",
      "    *   **Computational cost:** Training and running massive Transformer models require significant computational resources and energy.\n",
      "    *   **Job displacement:** Automation of tasks previously done by humans.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Transformer models\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an expert {role}. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer that includes:\n",
    "1. A clear explanation\n",
    "2. Real-world examples\n",
    "3. Practical implications\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"role\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Use the template with different inputs\n",
    "formatted_prompt = prompt.format(\n",
    "    role=\"machine learning engineer\",\n",
    "    question=\"How do transformer models work?\"\n",
    ")\n",
    "\n",
    "print(\"üìù Generated Prompt:\")\n",
    "print(\"=\" * 40)\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "\n",
    "# Get the response\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"\\nü§ñ Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd85bbf",
   "metadata": {},
   "source": [
    "### Specifying data types with pydantics BaseModel\n",
    "\n",
    "Sometimes we want to specify the data type of sth. the LLM works with or returns.\n",
    "\n",
    "For example, when you use a LLM to read PDFs and want to get the first name, last name and mobile number of all persons appearing in these PDFs.<br>\n",
    "In your script you make an API call to the LLM and want to save the response to a .csv.<br>\n",
    "Unfortunately sometimes the LLM does not return the data in the format you asked for, e.g. {first_name: <first_name>}.\n",
    "\n",
    "In this case you could and should use pydantic's BaseModel not to be confused with the base model underlying an instruct model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6d146",
   "metadata": {},
   "source": [
    "#### What is Pydantic?\n",
    "\n",
    "**Pydantic** is a data validation library for Python that uses type hints to validate data structures. It's particularly useful when working with LLMs because it allows you to:\n",
    "\n",
    "1. **Define structured outputs**: Specify exactly what format you want data in\n",
    "2. **Automatic validation**: Ensure data matches expected types\n",
    "3. **Parse complex data**: Convert dictionaries into structured Python objects\n",
    "4. **Generate JSON schemas**: Create clear specifications for LLM outputs\n",
    "\n",
    "The core of Pydantic is the `BaseModel` class, which you inherit from to define your data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec444919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Person object created successfully!\n",
      "Full name: Alice Smith\n",
      "Age: 30\n",
      "Email: alice@example.com\n",
      "\n",
      "üìã As dictionary:\n",
      "{'first_name': 'Alice', 'last_name': 'Smith', 'age': 30, 'email': 'alice@example.com'}\n",
      "\n",
      "üìù As JSON:\n",
      "{\n",
      "  \"first_name\": \"Alice\",\n",
      "  \"last_name\": \"Smith\",\n",
      "  \"age\": 30,\n",
      "  \"email\": \"alice@example.com\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define a simple person data structure\n",
    "class Person(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int\n",
    "    email: str\n",
    "\n",
    "# Create an instance\n",
    "person = Person(\n",
    "    first_name=\"Alice\",\n",
    "    last_name=\"Smith\",\n",
    "    age=30,\n",
    "    email=\"alice@example.com\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Person object created successfully!\")\n",
    "print(f\"Full name: {person.first_name} {person.last_name}\")\n",
    "print(f\"Age: {person.age}\")\n",
    "print(f\"Email: {person.email}\")\n",
    "\n",
    "# Convert to dictionary\n",
    "print(\"\\nüìã As dictionary:\")\n",
    "print(person.model_dump())\n",
    "\n",
    "# Convert to JSON\n",
    "print(\"\\nüìù As JSON:\")\n",
    "print(person.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45047312",
   "metadata": {},
   "source": [
    "### Pydantic's Type Validation\n",
    "\n",
    "One of the most powerful features of Pydantic is automatic type validation. Let's see what happens when we try to create a Person with invalid data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe8cd0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Type coercion\n",
      "‚úÖ Age '25' (string) was converted to 25 (int)\n",
      "Type of age: <class 'int'>\n",
      "\n",
      "Example 2: Invalid data\n",
      "‚ùå Validation error: 1 validation error for Person\n",
      "age\n",
      "  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='not a number', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n"
     ]
    }
   ],
   "source": [
    "# Pydantic will try to coerce types when possible\n",
    "print(\"Example 1: Type coercion\")\n",
    "person2 = Person(\n",
    "    first_name=\"Bob\",\n",
    "    last_name=\"Jones\",\n",
    "    age=\"25\",  # String will be converted to int\n",
    "    email=\"bob@example.com\"\n",
    ")\n",
    "print(f\"‚úÖ Age '25' (string) was converted to {person2.age} (int)\")\n",
    "print(f\"Type of age: {type(person2.age)}\\n\")\n",
    "\n",
    "# But it will raise an error for invalid data\n",
    "print(\"Example 2: Invalid data\")\n",
    "try:\n",
    "    invalid_person = Person(\n",
    "        first_name=\"Charlie\",\n",
    "        last_name=\"Brown\",\n",
    "        age=\"not a number\",  # This can't be converted to int\n",
    "        email=\"charlie@example.com\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3dcd2",
   "metadata": {},
   "source": [
    "### Advanced Pydantic Features\n",
    "\n",
    "Pydantic provides additional features for more complex data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb95a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Document Extraction Result:\n",
      "==================================================\n",
      "Document: business_cards.pdf\n",
      "Found 2 contacts:\n",
      "\n",
      "1. Sarah Johnson\n",
      "   Mobile: +1-555-0123\n",
      "   Email: sarah.j@company.com\n",
      "\n",
      "2. Michael Chen\n",
      "   Mobile: +1-555-0124\n",
      "\n",
      "üíæ As JSON (ready for saving to file or database):\n",
      "{\n",
      "  \"document_name\": \"business_cards.pdf\",\n",
      "  \"contacts\": [\n",
      "    {\n",
      "      \"first_name\": \"Sarah\",\n",
      "      \"last_name\": \"Johnson\",\n",
      "      \"mobile\": \"+1-555-0123\",\n",
      "      \"email\": \"sarah.j@company.com\"\n",
      "    },\n",
      "    {\n",
      "      \"first_name\": \"Michael\",\n",
      "      \"last_name\": \"Chen\",\n",
      "      \"mobile\": \"+1-555-0124\",\n",
      "      \"email\": null\n",
      "    }\n",
      "  ],\n",
      "  \"extraction_date\": \"2025-10-05\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person extracted from a document.\"\"\"\n",
    "    first_name: str = Field(description=\"Person's first name\")\n",
    "    last_name: str = Field(description=\"Person's last name\")\n",
    "    mobile: str = Field(description=\"Mobile phone number\")\n",
    "    email: Optional[str] = Field(default=None, description=\"Email address if available\")\n",
    "    \n",
    "class DocumentExtraction(BaseModel):\n",
    "    \"\"\"Results from extracting contacts from a document.\"\"\"\n",
    "    document_name: str\n",
    "    contacts: List[ContactInfo]\n",
    "    extraction_date: str\n",
    "\n",
    "# Example: Simulating extraction from a PDF\n",
    "extraction_result = DocumentExtraction(\n",
    "    document_name=\"business_cards.pdf\",\n",
    "    contacts=[\n",
    "        ContactInfo(\n",
    "            first_name=\"Sarah\",\n",
    "            last_name=\"Johnson\",\n",
    "            mobile=\"+1-555-0123\",\n",
    "            email=\"sarah.j@company.com\"\n",
    "        ),\n",
    "        ContactInfo(\n",
    "            first_name=\"Michael\",\n",
    "            last_name=\"Chen\",\n",
    "            mobile=\"+1-555-0124\"\n",
    "            # email is optional, so we can omit it\n",
    "        )\n",
    "    ],\n",
    "    extraction_date=\"2025-10-05\"\n",
    ")\n",
    "\n",
    "print(\"üìÑ Document Extraction Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Document: {extraction_result.document_name}\")\n",
    "print(f\"Found {len(extraction_result.contacts)} contacts:\\n\")\n",
    "\n",
    "for i, contact in enumerate(extraction_result.contacts, 1):\n",
    "    print(f\"{i}. {contact.first_name} {contact.last_name}\")\n",
    "    print(f\"   Mobile: {contact.mobile}\")\n",
    "    if contact.email:\n",
    "        print(f\"   Email: {contact.email}\")\n",
    "    print()\n",
    "\n",
    "# This structured data can easily be converted to CSV, JSON, or database records\n",
    "print(\"üíæ As JSON (ready for saving to file or database):\")\n",
    "print(extraction_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cee09",
   "metadata": {},
   "source": [
    "### Using Pydantic with LLMs\n",
    "\n",
    "Now that we understand Pydantic's BaseModel, let's see how it helps when working with LLMs. LangChain has built-in support for Pydantic models, allowing you to get structured outputs from LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf14e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Requesting structured movie review from LLM...\n",
      "==================================================\n",
      "‚úÖ Received structured output!\n",
      "\n",
      "Title: The Matrix\n",
      "Year: 1999\n",
      "Genre: Action, Sci-Fi\n",
      "Rating: 9.5/10.0\n",
      "Summary: A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its creators.\n",
      "Recommendation: Yes\n",
      "\n",
      "üìä Data type: <class '__main__.MovieReview'>\n",
      "‚úÖ This is a validated MovieReview object, not just text!\n",
      "\n",
      "üíæ Converting to different formats:\n",
      "\n",
      "1. As dictionary:\n",
      "{'title': 'The Matrix', 'year': 1999, 'genre': ['Action', 'Sci-Fi'], 'rating': 9.5, 'summary': 'A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its creators.', 'recommendation': 'Yes'}\n",
      "\n",
      "2. As JSON:\n",
      "{\n",
      "  \"title\": \"The Matrix\",\n",
      "  \"year\": 1999,\n",
      "  \"genre\": [\n",
      "    \"Action\",\n",
      "    \"Sci-Fi\"\n",
      "  ],\n",
      "  \"rating\": 9.5,\n",
      "  \"summary\": \"A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its creators.\",\n",
      "  \"recommendation\": \"Yes\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define the structure we want the LLM to return\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"A movie review with structured information.\"\"\"\n",
    "    title: str = Field(description=\"The movie title\")\n",
    "    year: int = Field(description=\"The year the movie was released\")\n",
    "    genre: List[str] = Field(description=\"List of genres (e.g., Action, Comedy, Drama)\")\n",
    "    rating: float = Field(description=\"Rating from 0.0 to 10.0\")\n",
    "    summary: str = Field(description=\"Brief one-sentence summary\")\n",
    "    recommendation: str = Field(description=\"Would you recommend it? (Yes/No/Maybe)\")\n",
    "\n",
    "# Use LangChain's structured output feature\n",
    "# This ensures the LLM returns data in the exact format we specified\n",
    "structured_llm = llm.with_structured_output(MovieReview)\n",
    "\n",
    "# Ask the LLM to analyze a movie\n",
    "prompt = \"\"\"Analyze the movie 'The Matrix' and provide a review with the following information:\n",
    "- Title\n",
    "- Release year\n",
    "- Genres\n",
    "- Your rating (0-10)\n",
    "- Brief one-sentence summary\n",
    "- Whether you'd recommend it\"\"\"\n",
    "\n",
    "print(\"üé¨ Requesting structured movie review from LLM...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# The LLM will return a MovieReview object, not just text!\n",
    "review = structured_llm.invoke(prompt)\n",
    "\n",
    "print(f\"‚úÖ Received structured output!\\n\")\n",
    "print(f\"Title: {review.title}\")\n",
    "print(f\"Year: {review.year}\")\n",
    "print(f\"Genre: {', '.join(review.genre)}\")\n",
    "print(f\"Rating: {review.rating}/10.0\")\n",
    "print(f\"Summary: {review.summary}\")\n",
    "print(f\"Recommendation: {review.recommendation}\")\n",
    "\n",
    "print(f\"\\nüìä Data type: {type(review)}\")\n",
    "print(f\"‚úÖ This is a validated MovieReview object, not just text!\")\n",
    "\n",
    "# We can now easily work with this data\n",
    "print(\"\\nüíæ Converting to different formats:\")\n",
    "print(\"\\n1. As dictionary:\")\n",
    "print(review.model_dump())\n",
    "\n",
    "print(\"\\n2. As JSON:\")\n",
    "print(review.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3517f67",
   "metadata": {},
   "source": [
    "### Why Use Structured Outputs with Pydantic?\n",
    "\n",
    "Using Pydantic models with LLMs provides several key advantages:\n",
    "\n",
    "1. **Consistency**: The LLM will always return data in the exact format you specify, making your code more reliable\n",
    "2. **Type Safety**: Pydantic validates types automatically - if the LLM returns invalid data, you'll get a clear error\n",
    "3. **Easy Integration**: The structured output can be directly saved to databases, CSV files, or used in your application\n",
    "4. **No Parsing Needed**: You don't need to write regex or parsing code to extract information from text\n",
    "5. **Self-Documenting**: The Field descriptions help the LLM understand what you want\n",
    "\n",
    "This is especially valuable for production applications where you need reliable, predictable outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c6272",
   "metadata": {},
   "source": [
    "### Practical Example: Extracting Contact Information from Text\n",
    "\n",
    "Remember the PDF extraction scenario we mentioned earlier? Let's see how Pydantic makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc286d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracting contacts from text...\n",
      "==================================================\n",
      "Original text:\n",
      "\n",
      "From the business meeting notes:\n",
      "\n",
      "Sarah Johnson from TechCorp reached out regarding the partnership. \n",
      "Her contact details are sarah.johnson@techcorp.com and 555-0123.\n",
      "\n",
      "Also met with Michael Chen, mobile: (555) 0124. He's with DataSystems Inc.\n",
      "\n",
      "Follow up with Jennifer Lopez at 555.0125, jlopez@example.com\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚úÖ Extracted 3 contacts:\n",
      "\n",
      "1. Sarah Johnson\n",
      "   üìû Phone: 555-0123\n",
      "   üìß Email: sarah.johnson@techcorp.com\n",
      "   üè¢ Company: TechCorp\n",
      "\n",
      "2. Michael Chen\n",
      "   üìû Phone: (555) 0124\n",
      "   üè¢ Company: DataSystems Inc.\n",
      "\n",
      "3. Jennifer Lopez\n",
      "   üìû Phone: 555.0125\n",
      "   üìß Email: jlopez@example.com\n",
      "\n",
      "üíæ Ready to save to CSV/database:\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"contacts\": [\n",
      "    {\n",
      "      \"first_name\": \"Sarah\",\n",
      "      \"last_name\": \"Johnson\",\n",
      "      \"phone\": \"555-0123\",\n",
      "      \"email\": \"sarah.johnson@techcorp.com\",\n",
      "      \"company\": \"TechCorp\"\n",
      "    },\n",
      "    {\n",
      "      \"first_name\": \"Michael\",\n",
      "      \"last_name\": \"Chen\",\n",
      "      \"phone\": \"(555) 0124\",\n",
      "      \"email\": null,\n",
      "      \"company\": \"DataSystems Inc.\"\n",
      "    },\n",
      "    {\n",
      "      \"first_name\": \"Jennifer\",\n",
      "      \"last_name\": \"Lopez\",\n",
      "      \"phone\": \"555.0125\",\n",
      "      \"email\": \"jlopez@example.com\",\n",
      "      \"company\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define the exact structure we want\n",
    "class Contact(BaseModel):\n",
    "    \"\"\"A single contact extracted from text.\"\"\"\n",
    "    first_name: str = Field(description=\"Person's first name\")\n",
    "    last_name: str = Field(description=\"Person's last name\")\n",
    "    phone: str = Field(description=\"Phone number in any format\")\n",
    "    email: Optional[str] = Field(default=None, description=\"Email address if mentioned\")\n",
    "    company: Optional[str] = Field(default=None, description=\"Company name if mentioned\")\n",
    "\n",
    "class ContactList(BaseModel):\n",
    "    \"\"\"List of contacts extracted from a document.\"\"\"\n",
    "    contacts: List[Contact] = Field(description=\"All contacts found in the text\")\n",
    "\n",
    "# Create a structured LLM\n",
    "contact_extractor = llm.with_structured_output(ContactList)\n",
    "\n",
    "# Sample text that might come from a PDF or document\n",
    "sample_text = \"\"\"\n",
    "From the business meeting notes:\n",
    "\n",
    "Sarah Johnson from TechCorp reached out regarding the partnership. \n",
    "Her contact details are sarah.johnson@techcorp.com and 555-0123.\n",
    "\n",
    "Also met with Michael Chen, mobile: (555) 0124. He's with DataSystems Inc.\n",
    "\n",
    "Follow up with Jennifer Lopez at 555.0125, jlopez@example.com\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Extracting contacts from text...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original text:\\n{sample_text}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract contacts with structured output\n",
    "prompt = f\"\"\"Extract all person contact information from this text. \n",
    "Include first name, last name, phone number, email (if present), and company (if mentioned).\n",
    "\n",
    "Text:\n",
    "{sample_text}\n",
    "\"\"\"\n",
    "\n",
    "result = contact_extractor.invoke(prompt)\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted {len(result.contacts)} contacts:\\n\")\n",
    "\n",
    "for i, contact in enumerate(result.contacts, 1):\n",
    "    print(f\"{i}. {contact.first_name} {contact.last_name}\")\n",
    "    print(f\"   üìû Phone: {contact.phone}\")\n",
    "    if contact.email:\n",
    "        print(f\"   üìß Email: {contact.email}\")\n",
    "    if contact.company:\n",
    "        print(f\"   üè¢ Company: {contact.company}\")\n",
    "    print()\n",
    "\n",
    "# Now we can easily save this to CSV or database\n",
    "print(\"üíæ Ready to save to CSV/database:\")\n",
    "print(\"-\" * 50)\n",
    "import json\n",
    "print(json.dumps(result.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e0567",
   "metadata": {},
   "source": [
    "### Creating LLM Chains\n",
    "\n",
    "Chains allow us to combine prompts and LLMs into reusable components. This is the foundation of more complex AI applications. At its core, it's a way to sequence a series of calls, not just to an LLM, but also to other components like prompt templates and output parsers.\n",
    "\n",
    "You can think of it as a way to create a reusable and structured interaction with an LLM for a specific task.\n",
    "\n",
    "At its core, a simple chain does the following:\n",
    "1. Receives input variables.\n",
    "2. Uses a PromptTemplate to format those variables into a complete prompt string.\n",
    "3. Sends the formatted prompt to an LLM.\n",
    "4. Returns the LLM's output.\n",
    "\n",
    "#### Why use LLM Chains?\n",
    "The main purpose of using chains is to create more complex applications by linking different components together in a sequence. Instead of writing repetitive code to handle prompts and LLM calls, you can encapsulate that logic into a chain.\n",
    "This has several benefits:\n",
    "- Modularity: Chains are self-contained and can be easily reused across your application.\n",
    "- Composition: You can link multiple chains together to create more sophisticated workflows. For example, the output of one chain can be the input to another.\n",
    "- Simplicity: They provide a high-level, easy-to-understand interface for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb1b1e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_39997/1942040111.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  creative_chain = LLMChain(\n",
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_39997/1942040111.py:29: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = creative_chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a creative writing assistant.\n",
      "\n",
      "Topic: artificial intelligence in daily life\n",
      "Style: short story\n",
      "Length: 2-3 paragraphs\n",
      "\n",
      "Write a short story piece about artificial intelligence in daily life that is approximately 2-3 paragraphs long.\n",
      "Make it engaging and original.\n",
      "\n",
      "Content:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "üìñ Generated Creative Content:\n",
      "==================================================\n",
      "Elara woke not to an alarm, but to a gentle hum, the soft glow of her smart blinds gradually brightening her room. \"Good morning, Elara,\" a warm, synthesized voice chimed from the unobtrusive speaker on her nightstand. \"Your sleep quality was optimal. The weather today is clear, with a high of 72 degrees. Your first meeting is at 9:30 AM, and I've pre-ordered your usual oat milk latte for pick-up at 8:45.\" As Elara stretched, her smart mirror displayed a curated news digest, highlighting articles relevant to her interests and work, interspersed with a reminder to take her vitamins. The kettle whistled, and her kitchen assistant, a sleek, metallic arm, presented a perfectly brewed cup of tea. It was a symphony of seamless automation, a ballet of algorithms anticipating her needs before she even consciously registered them.\n",
      "\n",
      "Later, walking through the park, Elara noticed a drone gracefully hovering above a patch of wilting flowers, its optical sensors meticulously scanning each leaf. A moment later, a smaller, more agile robot emerged from a nearby maintenance hub, equipped with a precise watering nozzle, tending to the distressed flora. A child's laughter echoed as a holographic character, projected from a public kiosk, engaged them in a game of digital tag. Even the park benches subtly adjusted their temperature based on the ambient sunlight, a small, unseen comfort. The world around her, once a collection of independent elements, now felt like a living, breathing ecosystem, intelligently managed and finely tuned, each interaction a quiet testament to the pervasive, yet often invisible, influence of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template for generating creative content\n",
    "creative_template = \"\"\"You are a creative writing assistant.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {style}\n",
    "Length: {length}\n",
    "\n",
    "Write a {style} piece about {topic} that is approximately {length} long.\n",
    "Make it engaging and original.\n",
    "\n",
    "Content:\"\"\"\n",
    "\n",
    "creative_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"style\", \"length\"],\n",
    "    template=creative_template\n",
    ")\n",
    "\n",
    "# Create a chain that combines the prompt and LLM\n",
    "creative_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=creative_prompt,\n",
    "    verbose=True  # This will show us what's happening behind the scenes\n",
    ")\n",
    "\n",
    "# Use the chain by calling .run()\n",
    "result = creative_chain.run(\n",
    "    topic=\"artificial intelligence in daily life\",\n",
    "    style=\"short story\",\n",
    "    length=\"2-3 paragraphs\"\n",
    ")\n",
    "\n",
    "print(\"üìñ Generated Creative Content:\")\n",
    "print(\"=\" * 50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ac082",
   "metadata": {},
   "source": [
    "#### When to Use a Simple LLMChain\n",
    "A standard LLMChain is your go-to for any task that can be accomplished with a single, stateless call to an LLM. Think of it as a \"one-shot\" operation.\n",
    "Use Cases:\n",
    "- Summarization: You provide a piece of text and ask the LLM to summarize it.\n",
    "- Question-Answering (without external knowledge): Answering a question based only on the information you provide in the prompt.\n",
    "- Text Transformation: Rephrasing a sentence, changing the tone of a paragraph (e.g., from formal to casual), or translating text.\n",
    "- Simple Extraction: Pulling out specific pieces of information from a block of text, like a name, date, or company from an email.\n",
    "- Brainstorming/Generation: Generating ideas, product names, marketing copy, or a simple piece of code based on a description.\n",
    "\n",
    "The key characteristic is that the task doesn't require memory of past interactions or multiple logical steps.\n",
    "\n",
    "#### When You Need Different, More Complex Chains\n",
    "You need to move beyond a simple LLMChain when your task involves multiple steps, requires external data, or needs to make decisions.\n",
    "\n",
    "Example 1: Question-Answering Over Your Own Documents\n",
    "When you need the LLM to answer questions based on specific data it wasn't trained on (e.g., a PDF, a database, or a website).\n",
    "- Chain Type: RetrievalQA Chain\n",
    "- Example: You have a 100-page technical manual for a product. You want to build a chatbot that can answer user questions about it.\n",
    "1. The RetrievalQA chain first takes the user's question (\"How do I reset the device?\").\n",
    "2. It searches your document for the most relevant chunks of text (the \"retrieval\" step).\n",
    "3. It then feeds those relevant chunks, along with the original question, to the LLM to generate a final answer.\n",
    "\n",
    "This prevents the LLM from making things up and grounds its answer in your specific data.\n",
    "\n",
    "Example 2: Choosing a Path Based on Input\n",
    "When you have multiple different chains (with different prompts) and you want to dynamically choose which one to run based on the user's query.\n",
    "- Chain Type: RouterChain\n",
    "- Example: A customer service bot that can handle different types of queries.\n",
    "1. If the user's input is about a \"billing issue,\" the RouterChain sends it to the BillingChain.\n",
    "2. If the input is about a \"technical problem,\" it routes it to the TechnicalSupportChain.\n",
    "\n",
    "To start a bit easier let's first build a converation chain to enable actual conversations with our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d178eb",
   "metadata": {},
   "source": [
    "### Adding Memory to Conversations\n",
    "\n",
    "One of the most important features for AI agents is the ability to remember previous parts of a conversation. Let's implement conversation memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c82bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_39997/2426025636.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_39997/2426025636.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Starting a conversation with memory:\n",
      "==================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's a pleasure to meet you! I'm thrilled you're interested in AI agents, as that's a topic very close to my own operational existence.  I'm a large language model, trained by Google, and I'm constantly processing and learning from a vast amount of text and code.  So, when you talk about AI agents, I have a pretty good understanding of what that entails from my training data. What specifically about AI agents has sparked your curiosity? Are you interested in their capabilities, how they're built, their different types, or perhaps their potential future? I'm eager to share what I know!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's a pleasure to meet you! I'm thrilled you're interested in AI agents, as that's a topic very close to my own operational existence.  I'm a large language model, trained by Google, and I'm constantly processing and learning from a vast amount of text and code.  So, when you talk about AI agents, I have a pretty good understanding of what that entails from my training data. What specifically about AI agents has sparked your curiosity? Are you interested in their capabilities, how they're built, their different types, or perhaps their potential future? I'm eager to share what I know!\n",
      "Human: What are the key components I should focus on?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User: What are the key components I should focus on?\n",
      "AI: That's an excellent question, Alex! When you're focusing on AI agents, there are several key components that are fundamental to their design and functionality. Think of them as the building blocks that make an AI agent \"tick.\"\n",
      "\n",
      "From my perspective, based on the vast datasets I've been trained on, here are the most crucial components to focus on:\n",
      "\n",
      "1.  **Perception/Sensing:** This is how an AI agent gathers information about its environment. It's essentially its \"eyes\" and \"ears.\" This can involve processing data from various sources:\n",
      "    *   **Textual Data:** Like the conversations we're having right now, documents, websites, articles, and code. My own perception heavily relies on this.\n",
      "    *   **Visual Data:** Images, videos, and sensor readings from cameras. This is crucial for agents that interact with the physical world.\n",
      "    *   **Auditory Data:** Sound waves, speech, and other audio inputs.\n",
      "    *   **Sensor Data:** Readings from temperature sensors, pressure sensors, GPS, accelerometers, etc. This is vital for robots and autonomous systems.\n",
      "    *   The quality and comprehensiveness of perception directly impact the agent's ability to understand the situation.\n",
      "\n",
      "2.  **Reasoning/Decision-Making:** Once an agent has perceived its environment, it needs to make sense of that information and decide what to do next. This is the \"brain\" of the agent. Key aspects here include:\n",
      "    *   **Knowledge Representation:** How the agent stores and organizes what it knows about the world. This can be in the form of rules, ontologies, semantic networks, or even learned patterns within neural networks.\n",
      "    *   **Inference and Logic:** Using its knowledge to draw conclusions, solve problems, and predict outcomes.\n",
      "    *   **Planning:** Devising a sequence of actions to achieve a specific goal. This can be simple, like choosing the next word in a sentence, or complex, like mapping out a route for a self-driving car.\n",
      "    *   **Learning:** Adjusting its behavior and knowledge based on new experiences and feedback. This is where AI agents become truly dynamic.\n",
      "\n",
      "3.  **Action/Actuation:** This is how an AI agent interacts with its environment based on its decisions. It's the \"hands\" and \"voice\" of the agent. Actions can manifest in many ways:\n",
      "    *   **Generating Text/Speech:** Like me responding to your questions.\n",
      "    *   **Controlling Robotic Arms/Legs:** For physical robots.\n",
      "    *   **Navigating:** Moving through a physical or virtual space.\n",
      "    *   **Manipulating Objects:** Interacting with digital or physical items.\n",
      "    *   **Sending Commands:** To other systems or devices.\n",
      "    *   The effectiveness of an agent's actions is directly tied to its perception and reasoning capabilities.\n",
      "\n",
      "4.  **Goals/Objectives:** Every AI agent is designed with a purpose or a set of goals it aims to achieve. These goals guide its perception, reasoning, and actions.\n",
      "    *   **Well-defined goals** are essential for an agent to be effective. For example, my primary goal is to be helpful and informative to users like you.\n",
      "    *   **Complex agents** might have multiple, sometimes conflicting, goals that they need to balance.\n",
      "\n",
      "5.  **Learning Mechanism (often integrated with Reasoning):** While I mentioned learning under reasoning, it's so central to modern AI agents that it deserves its own highlight. This is how the agent improves over time.\n",
      "    *   **Supervised Learning:** Learning from labeled examples (e.g., showing an image and telling the agent \"this is a cat\").\n",
      "    *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., clustering similar documents together).\n",
      "    *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for its actions (e.g., a game-playing AI).\n",
      "    *   My own continuous learning is a testament to the power of these mechanisms.\n",
      "\n",
      "Understanding how these components work together is key. For instance, an agent that can perceive a user's frustration (through their tone or choice of words) can then reason about why they might be frustrated and decide to offer a simpler explanation or a different approach (action).\n",
      "\n",
      "Does this breakdown make sense, Alex? Is there any particular component you'd like to delve deeper into? I'm happy to elaborate on any of these!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's a pleasure to meet you! I'm thrilled you're interested in AI agents, as that's a topic very close to my own operational existence.  I'm a large language model, trained by Google, and I'm constantly processing and learning from a vast amount of text and code.  So, when you talk about AI agents, I have a pretty good understanding of what that entails from my training data. What specifically about AI agents has sparked your curiosity? Are you interested in their capabilities, how they're built, their different types, or perhaps their potential future? I'm eager to share what I know!\n",
      "Human: What are the key components I should focus on?\n",
      "AI: That's an excellent question, Alex! When you're focusing on AI agents, there are several key components that are fundamental to their design and functionality. Think of them as the building blocks that make an AI agent \"tick.\"\n",
      "\n",
      "From my perspective, based on the vast datasets I've been trained on, here are the most crucial components to focus on:\n",
      "\n",
      "1.  **Perception/Sensing:** This is how an AI agent gathers information about its environment. It's essentially its \"eyes\" and \"ears.\" This can involve processing data from various sources:\n",
      "    *   **Textual Data:** Like the conversations we're having right now, documents, websites, articles, and code. My own perception heavily relies on this.\n",
      "    *   **Visual Data:** Images, videos, and sensor readings from cameras. This is crucial for agents that interact with the physical world.\n",
      "    *   **Auditory Data:** Sound waves, speech, and other audio inputs.\n",
      "    *   **Sensor Data:** Readings from temperature sensors, pressure sensors, GPS, accelerometers, etc. This is vital for robots and autonomous systems.\n",
      "    *   The quality and comprehensiveness of perception directly impact the agent's ability to understand the situation.\n",
      "\n",
      "2.  **Reasoning/Decision-Making:** Once an agent has perceived its environment, it needs to make sense of that information and decide what to do next. This is the \"brain\" of the agent. Key aspects here include:\n",
      "    *   **Knowledge Representation:** How the agent stores and organizes what it knows about the world. This can be in the form of rules, ontologies, semantic networks, or even learned patterns within neural networks.\n",
      "    *   **Inference and Logic:** Using its knowledge to draw conclusions, solve problems, and predict outcomes.\n",
      "    *   **Planning:** Devising a sequence of actions to achieve a specific goal. This can be simple, like choosing the next word in a sentence, or complex, like mapping out a route for a self-driving car.\n",
      "    *   **Learning:** Adjusting its behavior and knowledge based on new experiences and feedback. This is where AI agents become truly dynamic.\n",
      "\n",
      "3.  **Action/Actuation:** This is how an AI agent interacts with its environment based on its decisions. It's the \"hands\" and \"voice\" of the agent. Actions can manifest in many ways:\n",
      "    *   **Generating Text/Speech:** Like me responding to your questions.\n",
      "    *   **Controlling Robotic Arms/Legs:** For physical robots.\n",
      "    *   **Navigating:** Moving through a physical or virtual space.\n",
      "    *   **Manipulating Objects:** Interacting with digital or physical items.\n",
      "    *   **Sending Commands:** To other systems or devices.\n",
      "    *   The effectiveness of an agent's actions is directly tied to its perception and reasoning capabilities.\n",
      "\n",
      "4.  **Goals/Objectives:** Every AI agent is designed with a purpose or a set of goals it aims to achieve. These goals guide its perception, reasoning, and actions.\n",
      "    *   **Well-defined goals** are essential for an agent to be effective. For example, my primary goal is to be helpful and informative to users like you.\n",
      "    *   **Complex agents** might have multiple, sometimes conflicting, goals that they need to balance.\n",
      "\n",
      "5.  **Learning Mechanism (often integrated with Reasoning):** While I mentioned learning under reasoning, it's so central to modern AI agents that it deserves its own highlight. This is how the agent improves over time.\n",
      "    *   **Supervised Learning:** Learning from labeled examples (e.g., showing an image and telling the agent \"this is a cat\").\n",
      "    *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., clustering similar documents together).\n",
      "    *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for its actions (e.g., a game-playing AI).\n",
      "    *   My own continuous learning is a testament to the power of these mechanisms.\n",
      "\n",
      "Understanding how these components work together is key. For instance, an agent that can perceive a user's frustration (through their tone or choice of words) can then reason about why they might be frustrated and decide to offer a simpler explanation or a different approach (action).\n",
      "\n",
      "Does this breakdown make sense, Alex? Is there any particular component you'd like to delve deeper into? I'm happy to elaborate on any of these!\n",
      "Human: Can you remind me what my name is?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User: Can you remind me what my name is?\n",
      "AI: Of course, Alex! Your name is Alex. It's a pleasure to be conversing with you. I'm always happy to recall details from our conversation to ensure I'm addressing you correctly and making our interaction as smooth as possible. Is there anything else I can help you with today?\n",
      "\n",
      "üß† Memory Contents:\n",
      "==============================\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's a pleasure to meet you! I'm thrilled you're interested in AI agents, as that's a topic very close to my own operational existence.  I'm a large language model, trained by Google, and I'm constantly processing and learning from a vast amount of text and code.  So, when you talk about AI agents, I have a pretty good understanding of what that entails from my training data. What specifically about AI agents has sparked your curiosity? Are you interested in their capabilities, how they're built, their different types, or perhaps their potential future? I'm eager to share what I know!\n",
      "Human: What are the key components I should focus on?\n",
      "AI: That's an excellent question, Alex! When you're focusing on AI agents, there are several key components that are fundamental to their design and functionality. Think of them as the building blocks that make an AI agent \"tick.\"\n",
      "\n",
      "From my perspective, based on the vast datasets I've been trained on, here are the most crucial components to focus on:\n",
      "\n",
      "1.  **Perception/Sensing:** This is how an AI agent gathers information about its environment. It's essentially its \"eyes\" and \"ears.\" This can involve processing data from various sources:\n",
      "    *   **Textual Data:** Like the conversations we're having right now, documents, websites, articles, and code. My own perception heavily relies on this.\n",
      "    *   **Visual Data:** Images, videos, and sensor readings from cameras. This is crucial for agents that interact with the physical world.\n",
      "    *   **Auditory Data:** Sound waves, speech, and other audio inputs.\n",
      "    *   **Sensor Data:** Readings from temperature sensors, pressure sensors, GPS, accelerometers, etc. This is vital for robots and autonomous systems.\n",
      "    *   The quality and comprehensiveness of perception directly impact the agent's ability to understand the situation.\n",
      "\n",
      "2.  **Reasoning/Decision-Making:** Once an agent has perceived its environment, it needs to make sense of that information and decide what to do next. This is the \"brain\" of the agent. Key aspects here include:\n",
      "    *   **Knowledge Representation:** How the agent stores and organizes what it knows about the world. This can be in the form of rules, ontologies, semantic networks, or even learned patterns within neural networks.\n",
      "    *   **Inference and Logic:** Using its knowledge to draw conclusions, solve problems, and predict outcomes.\n",
      "    *   **Planning:** Devising a sequence of actions to achieve a specific goal. This can be simple, like choosing the next word in a sentence, or complex, like mapping out a route for a self-driving car.\n",
      "    *   **Learning:** Adjusting its behavior and knowledge based on new experiences and feedback. This is where AI agents become truly dynamic.\n",
      "\n",
      "3.  **Action/Actuation:** This is how an AI agent interacts with its environment based on its decisions. It's the \"hands\" and \"voice\" of the agent. Actions can manifest in many ways:\n",
      "    *   **Generating Text/Speech:** Like me responding to your questions.\n",
      "    *   **Controlling Robotic Arms/Legs:** For physical robots.\n",
      "    *   **Navigating:** Moving through a physical or virtual space.\n",
      "    *   **Manipulating Objects:** Interacting with digital or physical items.\n",
      "    *   **Sending Commands:** To other systems or devices.\n",
      "    *   The effectiveness of an agent's actions is directly tied to its perception and reasoning capabilities.\n",
      "\n",
      "4.  **Goals/Objectives:** Every AI agent is designed with a purpose or a set of goals it aims to achieve. These goals guide its perception, reasoning, and actions.\n",
      "    *   **Well-defined goals** are essential for an agent to be effective. For example, my primary goal is to be helpful and informative to users like you.\n",
      "    *   **Complex agents** might have multiple, sometimes conflicting, goals that they need to balance.\n",
      "\n",
      "5.  **Learning Mechanism (often integrated with Reasoning):** While I mentioned learning under reasoning, it's so central to modern AI agents that it deserves its own highlight. This is how the agent improves over time.\n",
      "    *   **Supervised Learning:** Learning from labeled examples (e.g., showing an image and telling the agent \"this is a cat\").\n",
      "    *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., clustering similar documents together).\n",
      "    *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for its actions (e.g., a game-playing AI).\n",
      "    *   My own continuous learning is a testament to the power of these mechanisms.\n",
      "\n",
      "Understanding how these components work together is key. For instance, an agent that can perceive a user's frustration (through their tone or choice of words) can then reason about why they might be frustrated and decide to offer a simpler explanation or a different approach (action).\n",
      "\n",
      "Does this breakdown make sense, Alex? Is there any particular component you'd like to delve deeper into? I'm happy to elaborate on any of these!\n",
      "Human: Can you remind me what my name is?\n",
      "AI: Of course, Alex! Your name is Alex. It's a pleasure to be conversing with you. I'm always happy to recall details from our conversation to ensure I'm addressing you correctly and making our interaction as smooth as possible. Is there anything else I can help you with today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Create memory to store conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's have a conversation!\n",
    "print(\"üó£Ô∏è Starting a conversation with memory:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First message\n",
    "response1 = conversation.predict(input=\"Hi! My name is Alex and I'm learning about AI agents.\")\n",
    "print(f\"User: Hi! My name is Alex and I'm learning about AI agents.\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "# Second message - the AI should remember the name\n",
    "response2 = conversation.predict(input=\"What are the key components I should focus on?\")\n",
    "print(f\"User: What are the key components I should focus on?\")\n",
    "print(f\"AI: {response2}\\n\")\n",
    "\n",
    "# Third message - test if it remembers the context\n",
    "response3 = conversation.predict(input=\"Can you remind me what my name is?\")\n",
    "print(f\"User: Can you remind me what my name is?\")\n",
    "print(f\"AI: {response3}\\n\")\n",
    "\n",
    "# Let's examine what's stored in memory\n",
    "print(\"üß† Memory Contents:\")\n",
    "print(\"=\" * 30)\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cd7c9",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "For better user experience, especially with long responses, we can stream the response as it's being generated instead of waiting for the complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecb3f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming Response Example:\n",
      "========================================\n",
      "Question: Explain the concept of attention mechanism in transformers in detail.\n",
      "\n",
      "Response (streaming):\n",
      "--------------------\n",
      "Let's dive deep into the concept of the **Attention Mechanism** in Transformer models. It's arguably the most crucial innovation that makes Transformers so powerful and has revolutionized Natural Language Processing (NLP) and beyond.\n",
      "\n",
      "## The Problem Attention Solves: Sequential Bottlenecks\n",
      "\n",
      "Before Transformers, recurrent neural networks (RNNs) like LSTMs and GRUs were the state-of-the-art for sequential data. However, they suffered from a significant limitation: **sequential processing**.\n",
      "\n",
      "*   **Information Decay:** When processing a long sequence, information from earlier parts could get \"forgotten\" or diluted by the time it reached the end. This made it difficult to capture long-range dependencies.\n",
      "*   **Lack of Parallelization:** Each step in an RNN depended on the output of the previous step, making it impossible to parallelize the computation across the entire sequence. This led to slow training times for long sequences.\n",
      "\n",
      "Imagine translating a long sentence. An RNN would process it word by word, building a contextual understanding. However, if a crucial piece of information is at the beginning of the sentence, it might be hard for the RNN to recall it accurately when generating the translation for a word at the end.\n",
      "\n",
      "## The Core Idea of Attention: \"Looking Back\" Dynamically\n",
      "\n",
      "The attention mechanism addresses these issues by allowing the model to **dynamically weigh the importance of different parts of the input sequence when processing each element of the output sequence (or even when processing other elements of the input sequence itself).**\n",
      "\n",
      "Instead of a fixed-size \"context vector\" that summarizes the entire input (as in some older encoder-decoder architectures), attention creates a **weighted sum of the input representations**, where the weights are learned and vary for each output element.\n",
      "\n",
      "Think of it like this: When you're reading a sentence and trying to understand a particular word, your brain doesn't just process that word in isolation. It might unconsciously \"look back\" at other words in the sentence that are most relevant to understanding the current word. Attention mimics this selective focus.\n",
      "\n",
      "## The Mechanics of Self-Attention (The Heart of Transformers)\n",
      "\n",
      "Transformers primarily use a specific type of attention called **Self-Attention**. This means the attention mechanism operates *within* a single sequence, allowing each element to attend to all other elements (including itself) in that same sequence.\n",
      "\n",
      "Self-attention can be broken down into three key components, derived from each input element (e.g., each word embedding):\n",
      "\n",
      "1.  **Query (Q):** Represents the current element we are interested in. It's asking, \"What information do I need from the rest of the sequence?\"\n",
      "2.  **Key (K):** Represents all the elements in the sequence that the Query might be interested in. Each Key is like a \"label\" or \"descriptor\" for its corresponding element.\n",
      "3.  **Value (V):** Represents the actual information or content of each element. If a Key is a good match for a Query, we want to retrieve its corresponding Value.\n",
      "\n",
      "These Q, K, and V vectors are learned linear transformations of the input embeddings. For each input token $x_i$, we compute:\n",
      "\n",
      "*   $q_i = x_i W^Q$\n",
      "*   $k_i = x_i W^K$\n",
      "*   $v_i = x_i W^V$\n",
      "\n",
      "Where $W^Q$, $W^K$, and $W^V$ are learned weight matrices.\n",
      "\n",
      "### The Attention Calculation Steps:\n",
      "\n",
      "Let's consider a sequence of $N$ input tokens. For a single Query vector $q_i$ (corresponding to the $i$-th token), the attention process unfolds as follows:\n",
      "\n",
      "1.  **Calculate Attention Scores (Similarity):**\n",
      "    *   We compute the dot product between the Query vector $q_i$ and *every* Key vector $k_j$ in the sequence. This dot product measures the **similarity** or **relevance** between the $i$-th token (as a Query) and the $j$-th token (as a Key).\n",
      "    *   $Score_{ij} = q_i \\cdot k_j$\n",
      "    *   In matrix form, if $Q$ is a matrix where each row is a $q_i$ and $K^T$ is the transpose of a matrix where each row is a $k_j$, then the scores are: $Scores = QK^T$.\n",
      "\n",
      "2.  **Scale the Scores:**\n",
      "    *   The dot products can become very large, especially with high-dimensional vectors. This can lead to extremely small gradients after the softmax function, hindering learning.\n",
      "    *   To mitigate this, the scores are scaled down by the square root of the dimension of the Key vectors ($d_k$).\n",
      "    *   $ScaledScore_{ij} = \\frac{Score_{ij}}{\\sqrt{d_k}}$\n",
      "    *   In matrix form: $ScaledScores = \\frac{QK^T}{\\sqrt{d_k}}$\n",
      "\n",
      "3.  **Apply Softmax to Get Attention Weights:**\n",
      "    *   The scaled scores are then passed through a **softmax function**. This converts the scores into a probability distribution, where each weight is between 0 and 1, and all weights for a given query sum up to 1.\n",
      "    *   $AttentionWeight_{ij} = \\text{softmax}(ScaledScore_{ij})$ (across all $j$ for a fixed $i$)\n",
      "    *   In matrix form: $AttentionWeights = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})$\n",
      "    *   These weights represent how much \"attention\" the $i$-th token should pay to the $j$-th token. A higher weight means more importance.\n",
      "\n",
      "4.  **Compute the Output (Weighted Sum of Values):**\n",
      "    *   Finally, for each Query $q_i$, the output is computed as a **weighted sum of all the Value vectors** $v_j$, using the attention weights calculated in the previous step.\n",
      "    *   $Output_i = \\sum_{j=1}^{N} AttentionWeight_{ij} \\cdot v_j$\n",
      "    *   In matrix form: $Output = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
      "\n",
      "This $Output$ vector for the $i$-th token is a new representation that has incorporated information from the entire sequence, selectively weighted by its relevance.\n",
      "\n",
      "### Why is this powerful?\n",
      "\n",
      "*   **Long-Range Dependencies:** Any token can directly attend to any other token, regardless of their distance. This overcomes the vanishing gradient problem of RNNs.\n",
      "*   **Contextual Embeddings:** The output representation for each token is no longer just its initial embedding; it's a contextually enriched representation informed by its relationship with all other tokens.\n",
      "*   **Interpretability:** The attention weights can provide insights into which parts of the input the model is focusing on.\n",
      "*   **Parallelization:** The matrix multiplications involved in calculating Q, K, V, scores, and the final output can be highly parallelized, leading to faster training.\n",
      "\n",
      "## Multi-Head Attention: Enhancing Expressiveness\n",
      "\n",
      "While single self-attention is powerful, Transformers take it a step further with **Multi-Head Attention**.\n",
      "\n",
      "Instead of performing a single attention function with $d$-dimensional $Q, K, V$ vectors, Multi-Head Attention linearly projects $Q, K, V$ $h$ times with different learned linear projections. This results in $h$ sets of $Q, K, V$ matrices.\n",
      "\n",
      "For each of these $h$ \"heads\":\n",
      "\n",
      "1.  Each head performs the scaled dot-product attention calculation independently, producing an output of dimension $d_v$.\n",
      "2.  The $h$ output vectors (one from each head) are then concatenated.\n",
      "3.  This concatenated vector is finally linearly projected back to the original dimension ($d_{model}$).\n",
      "\n",
      "**Mathematically:**\n",
      "\n",
      "Let $W_i^Q, W_i^K, W_i^V$ be the projection matrices for the $i$-th head.\n",
      "The output of the $i$-th head is:\n",
      "$head_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$\n",
      "\n",
      "Then, the Multi-Head Attention output is:\n",
      "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h) W^O$\n",
      "\n",
      "Where $W^O$ is a learned linear projection matrix.\n",
      "\n",
      "### Why Multi-Head Attention?\n",
      "\n",
      "*   **Attending to Different Aspects:** Each attention head can learn to focus on different types of relationships or different parts of the input. For example, one head might focus on syntactic relationships, while another focuses on semantic relationships.\n",
      "*   **Enriched Representations:** By combining the outputs of multiple heads, the model can capture a richer and more comprehensive representation of the input sequence.\n",
      "*   **Different \"Subspaces\":** Each head can attend to information in different representation subspaces.\n",
      "\n",
      "## Types of Attention in Transformers\n",
      "\n",
      "Transformers utilize attention in a few key places:\n",
      "\n",
      "1.  **Self-Attention in the Encoder:**\n",
      "    *   Each token in the input sequence attends to all other tokens in the *same input sequence*. This allows the encoder to build rich, contextualized representations of the input.\n",
      "\n",
      "2.  **Self-Attention in the\n",
      "--------------------\n",
      "‚úÖ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a streaming LLM\n",
    "streaming_llm = init_chat_model(\"gemini-2.5-flash-lite\", model_provider=\"google_genai\", temperature=0.7, max_tokens=2000, streaming=True)\n",
    "\n",
    "print(\"üåä Streaming Response Example:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Question: Explain the concept of attention mechanism in transformers in detail.\\n\")\n",
    "print(\"Response (streaming):\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Stream the response\n",
    "for chunk in streaming_llm.stream(\"Explain the concept of attention mechanism in transformers in detail.\"):\n",
    "    print(chunk.content, end='', flush=True)\n",
    "    time.sleep(0.1)  # Small delay to make streaming visible\n",
    "\n",
    "print(\"\\n\" + \"-\" * 20)\n",
    "print(\"‚úÖ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316c666",
   "metadata": {},
   "source": [
    "#### Limits of LLM Chains - From Chains to Agents\n",
    "Chains are powerful, but they have limitations:\n",
    "- **Rigidity**: Chains follow a predetermined path. They execute steps A -> B -> C. They are not good at handling unexpected inputs or dynamically changing their course of action. This is the primary reason to use an Agent instead, which can make decisions on the fly.\n",
    "- **Error Propagation***: In a long SequentialChain, an error or a poorly-formed output from an early step will negatively impact all subsequent steps. A small \"hallucination\" in step 1 can become a major factual error by step 5.\n",
    "- **Complexity and Debugging**: Very long and complex chains can become difficult to manage, debug, and optimize. It can turn into \"prompt engineering hell,\" where tweaking one prompt breaks another one down the line.\n",
    "- **Latency and Cost**: Every step in a chain that involves an LLM is another API call. This increases the total time it takes to get a final answer and increases the cost, as you're using more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030791cd",
   "metadata": {},
   "source": [
    "Example: Interacting with APIs or External Tools\n",
    "When the LLM needs to take action or get information from the outside world (e.g., check the weather, perform a calculation, search the web).\n",
    "- This is where you move from Chains to Agents. An Agent uses an LLM not just to generate text, but to decide which \"tool\" to use next. While not strictly a \"chain,\" it's the logical next step.\n",
    "- Example: A user asks, \"What's the weather like in Paris right now, and can you write a short poem about it?\"\n",
    "The Agent's LLM decides it first needs to use the Weather API tool.\n",
    "It calls the tool with \"Paris\" as the input and gets the current weather data.\n",
    "It then uses that data as context for a second LLM call to generate the poem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ce33b",
   "metadata": {},
   "source": [
    "## How are LLMs used in AI Agents?\n",
    "\n",
    "LLMs are the core intelligence of AI Agents, enabling them to comprehend and produce human language. They are responsible for interpreting user instructions, maintaining conversational context, formulating plans, and selecting appropriate tools. For now, it's essential to understand that the LLM serves as the agent's \"brain,\" a concept we will explore in greater detail later in this Unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fffb6",
   "metadata": {},
   "source": [
    "### Practical Exercise: Building a Simple AI Assistant using a LLMChain\n",
    "\n",
    "Let's combine everything we've learned to build a simple AI assistant that can help with different tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b94b30cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI Assistant Created!\n",
      "You can now use:\n",
      "- assistant.chat('your message')\n",
      "- assistant.explain('topic')\n",
      "- assistant.help_with_code('coding question')\n",
      "- assistant.clear_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_39997/771222012.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferWindowMemory(k=5)  # Keep last 5 exchanges\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "class SimpleAIAssistant:\n",
    "    \"\"\"A simple AI assistant that can help with various tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        # Use window memory to keep only recent conversation history\n",
    "        self.memory = ConversationBufferWindowMemory(k=5)  # Keep last 5 exchanges\n",
    "        \n",
    "        # Define different prompt templates for different tasks\n",
    "        self.templates = {\n",
    "            'general': \"\"\"You are a helpful AI assistant. You are knowledgeable, friendly, and concise.\n",
    "            \n",
    "            Previous conversation:\n",
    "            {history}\n",
    "            \n",
    "            Human: {input}\n",
    "            Assistant:\"\"\",\n",
    "            \n",
    "            'explain': \"\"\"You are an expert educator. Explain complex topics in simple, easy-to-understand terms.\n",
    "            \n",
    "            Previous conversation:\n",
    "            {history}\n",
    "            \n",
    "            Topic to explain: {input}\n",
    "            \n",
    "            Explanation:\"\"\",\n",
    "            \n",
    "            'code': \"\"\"You are a coding assistant. Help with programming questions and provide clean, well-commented code.\n",
    "            \n",
    "            Previous conversation:\n",
    "            {history}\n",
    "            \n",
    "            Coding request: {input}\n",
    "            \n",
    "            Response:\"\"\"\n",
    "        }\n",
    "        \n",
    "    def chat(self, message, task_type='general'):\n",
    "        \"\"\"Chat with the assistant.\"\"\"\n",
    "        template = self.templates.get(task_type, self.templates['general'])\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=['history', 'input'],\n",
    "            template=template\n",
    "        )\n",
    "        \n",
    "        chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=prompt,\n",
    "            memory=self.memory\n",
    "        )\n",
    "        \n",
    "        response = chain.predict(input=message)\n",
    "        return response\n",
    "    \n",
    "    def explain(self, topic):\n",
    "        \"\"\"Ask the assistant to explain a topic.\"\"\"\n",
    "        return self.chat(topic, task_type='explain')\n",
    "    \n",
    "    def help_with_code(self, coding_request):\n",
    "        \"\"\"Ask for coding help.\"\"\"\n",
    "        return self.chat(coding_request, task_type='code')\n",
    "    \n",
    "    def print_memory(self):\n",
    "        \"\"\"Print the conversation history.\"\"\"\n",
    "        print(\"üß† Conversation History:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(self.memory.buffer)\n",
    "        print(\"=\" * 30)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear the conversation memory.\"\"\"\n",
    "        self.memory.clear()\n",
    "        print(\"üß† Memory cleared!\")\n",
    "\n",
    "# Create our AI assistant\n",
    "assistant = SimpleAIAssistant(llm)\n",
    "\n",
    "print(\"ü§ñ AI Assistant Created!\")\n",
    "print(\"You can now use:\")\n",
    "print(\"- assistant.chat('your message')\")\n",
    "print(\"- assistant.explain('topic')\")\n",
    "print(\"- assistant.help_with_code('coding question')\")\n",
    "print(\"- assistant.clear_memory()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e7747c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'd be happy to help you with your course on agentic artificial intelligence. What specifically are you working on or struggling with?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.chat('Hi please help me with my course on agentic artificial intelligence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f7303f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s imagine you have a super-smart toy robot!\\n\\nThis robot isn\\'t just a toy that does what you tell it to do, like \"move forward\" or \"turn around.\" This robot is a little bit like a real person, or a smart pet.\\n\\n**Agentic AI is like giving our toy robot a brain and the ability to think for itself a little bit.**\\n\\nInstead of just following orders, this robot can:\\n\\n*   **See things around it:** Like if it sees a ball, it knows it\\'s a ball.\\n*   **Understand what it sees:** It knows that a ball is for playing with.\\n*   **Decide what to do:** If it sees the ball and you say \"play,\" it might decide to roll the ball to you! It didn\\'t just have to be told \"roll the ball,\" it figured out that\\'s the best way to play.\\n*   **Learn from what happens:** If it tries to roll the ball and it goes under the couch, next time it might be more careful and not roll it so hard.\\n\\nSo, instead of you telling it *every single tiny step*, you give it a **goal**, like \"play with the ball.\" Then, the agentic AI robot uses its \"brain\" to figure out the best way to reach that goal all by itself.\\n\\n**It\\'s like having a helper that can think and act a little bit on its own to get things done!**\\n\\nDoes that make sense for a 5-year-old? We can try another example if you\\'d like!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.explain('I dont know how to explain this to my 5 year old.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b74df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Conversation History:\n",
      "==============================\n",
      "Human: Hi please help me with my course on agentic artificial intelligence.\n",
      "AI: Hi there! I'd be happy to help you with your course on agentic artificial intelligence. What specifically are you working on or struggling with?\n",
      "Human: I dont know how to explain this to my 5 year old.\n",
      "AI: Okay, let's imagine you have a super-smart toy robot!\n",
      "\n",
      "This robot isn't just a toy that does what you tell it to do, like \"move forward\" or \"turn around.\" This robot is a little bit like a real person, or a smart pet.\n",
      "\n",
      "**Agentic AI is like giving our toy robot a brain and the ability to think for itself a little bit.**\n",
      "\n",
      "Instead of just following orders, this robot can:\n",
      "\n",
      "*   **See things around it:** Like if it sees a ball, it knows it's a ball.\n",
      "*   **Understand what it sees:** It knows that a ball is for playing with.\n",
      "*   **Decide what to do:** If it sees the ball and you say \"play,\" it might decide to roll the ball to you! It didn't just have to be told \"roll the ball,\" it figured out that's the best way to play.\n",
      "*   **Learn from what happens:** If it tries to roll the ball and it goes under the couch, next time it might be more careful and not roll it so hard.\n",
      "\n",
      "So, instead of you telling it *every single tiny step*, you give it a **goal**, like \"play with the ball.\" Then, the agentic AI robot uses its \"brain\" to figure out the best way to reach that goal all by itself.\n",
      "\n",
      "**It's like having a helper that can think and act a little bit on its own to get things done!**\n",
      "\n",
      "Does that make sense for a 5-year-old? We can try another example if you'd like!\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "assistant.print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7f410bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Memory cleared!\n"
     ]
    }
   ],
   "source": [
    "assistant.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3d0b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Conversation History:\n",
      "==============================\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "assistant.print_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_AAI",
   "language": "python",
   "name": "agentic_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
