{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0587917",
   "metadata": {},
   "source": [
    "# Agentic Artificial Intelligence\n",
    "## Exercise - Unit 02: Large Language Models\n",
    "\n",
    "Welcome to the second unit of the Agentic Artificial Intelligence course! \n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this unit, you will:\n",
    "1. understand tokenizers for language\n",
    "2. understand chat templates and prompt templates\n",
    "3. understand how to build simple LLM applications using chains\n",
    "4. build your first (simple) LLM powered AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134ab94",
   "metadata": {},
   "source": [
    "## What are LLMs?\n",
    "Large Language Models (LLMs) are AI models proficient in comprehending and producing human language. Trained on extensive text datasets, they acquire linguistic patterns, structures, and nuances, and are typically characterized by millions of parameters.\n",
    "\n",
    "The majority of contemporary LLMs leverage the Transformer architecture, a deep learning model rooted in the \"Attention\" mechanism, which has garnered considerable attention since Google's introduction of BERT in 2018.\n",
    "\n",
    "LLMs operate on a straightforward, highly effective principle: predicting the subsequent token based on a preceding sequence. A \"token\" is the fundamental unit of information an LLM processes, analogous to a word but optimized for efficiency, often representing sub-word units. For example, an LLM's vocabulary might be significantly smaller than the total number of words in a language (e.g., Llama 2 with ~32,000 tokens), utilizing combinable sub-word tokens like \"interest\" and \"ing\" to form \"interesting,\" or appending \"ed\" for \"interested.\" You can explore various tokenizers in the interactive playground below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07071607",
   "metadata": {},
   "source": [
    "### Experimenting with tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cde9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!'\n",
      "Token IDs: [101, 19204, 3989, 2003, 17160, 1998, 1996, 2607, 4005, 2594, 7976, 4454, 2003, 22371, 999, 102]\n",
      "Tokens: ['[CLS]', 'token', '##ization', 'is', 'fascinating', 'and', 'the', 'course', 'agent', '##ic', 'artificial', 'intelligence', 'is', 'exiting', '!', '[SEP]']\n",
      "Decoded Text: '[CLS] tokenization is fascinating and the course agentic artificial intelligence is exiting! [SEP]'\n"
     ]
    }
   ],
   "source": [
    "# You might need to install the libraries if you haven't already\n",
    "# pip install transformers torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "text_to_tokenize = \"Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!\"\n",
    "\n",
    "\n",
    "def tokenize_text(text_to_tokenize, tokenizer_name = \"bert-base-uncased\"):\n",
    "\n",
    "    # Load a tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # 1. Encode the text into token IDs\n",
    "    encoded_ids = tokenizer.encode(text_to_tokenize)\n",
    "    print(f\"Original Text: '{text_to_tokenize}'\")\n",
    "    print(f\"Token IDs: {encoded_ids}\")\n",
    "\n",
    "    # 2. Convert the IDs back into tokens (the actual text pieces)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_ids)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "\n",
    "    # 3. Decode the IDs back to the original string\n",
    "    decoded_text = tokenizer.decode(encoded_ids)\n",
    "    print(f\"Decoded Text: '{decoded_text}'\")\n",
    "\n",
    "tokenize_text(text_to_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7c4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!'\n",
      "Token IDs: [30642, 1634, 318, 13899, 290, 262, 1781, 15906, 291, 35941, 9345, 318, 33895, 0]\n",
      "Tokens: ['Token', 'ization', 'Ġis', 'Ġfascinating', 'Ġand', 'Ġthe', 'Ġcourse', 'ĠAgent', 'ic', 'ĠArtificial', 'ĠIntelligence', 'Ġis', 'Ġexiting', '!']\n",
      "Decoded Text: 'Tokenization is fascinating and the course Agentic Artificial Intelligence is exiting!'\n"
     ]
    }
   ],
   "source": [
    "tokenize_text(text_to_tokenize, 'gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167de6c",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) utilize special, model-specific tokens to structure their generated output and input prompts. These tokens delineate components like sequences, messages, and responses, with the End-of-Sequence (EOS) token being particularly crucial. The exact form and variety of these special tokens differ significantly across different LLM providers, as further illustrated in the table below.\n",
    "\n",
    "| Model | Provider | EOS Token | Functionality |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **GPT4** | OpenAI | `<endoftext>` | End of message text |\n",
    "| **Llama 3** | Meta (Facebook AI Research) | `<\\|eot_id\\|>` | End of sequence |\n",
    "| **Deepseek-R1** | DeepSeek | `<\\|end_of_sentence\\|>` | End of message text |\n",
    "| **SmolLM2** | Hugging Face | `<\\|im_end\\|>` | End of instruction or message |\n",
    "| **Gemma** | Google | `<end_of_turn>` | End of conversation turn |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce07ac",
   "metadata": {},
   "source": [
    "### Next token prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff1e74",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) operate autoregressively, where each generated token serves as input for predicting the subsequent token. This iterative process continues until the model generates an End-of-Sequence (EOS) token, signaling the completion of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0b747",
   "metadata": {},
   "source": [
    "<img src=\"AutoregressionSchema.gif\" alt=\"Alt text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db16fb",
   "metadata": {},
   "source": [
    "An LLM decodes text iteratively until it encounters the End-of-Sequence (EOS) token. During each decoding loop:\n",
    "\n",
    "1.  **Input Processing:** The input text is first tokenized. The model then generates a comprehensive representation of this token sequence, encoding both the meaning and positional information of each token.\n",
    "2.  **Likelihood Scoring:** This sequence representation is fed into the model, which subsequently outputs scores indicating the probability of every token in its vocabulary being the next in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b098f37",
   "metadata": {},
   "source": [
    "<img src=\"DecodingFinal.gif\" alt=\"Alt text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4f93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://agents-course-decoding-visualizer.hf.space ✔\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from gradio_client import Client\n",
    "\n",
    "def get_decoding_visualization(input_text):\n",
    "    \"\"\"\n",
    "    Calls the remote Gradio app and returns the HTML visualization.\n",
    "    \"\"\"\n",
    "    client = Client(\"agents-course/decoding_visualizer\")\n",
    "    result = client.predict(\n",
    "        input_text=input_text,\n",
    "        api_name=\"/get_beam_search_html\"\n",
    "    )\n",
    "    \n",
    "    # The client.predict() result may be a tuple; handle it to get the string.\n",
    "    if isinstance(result, tuple):\n",
    "        html_string = result[0]\n",
    "    else:\n",
    "        html_string = result\n",
    "        \n",
    "    return gr.HTML(value=html_string)\n",
    "\n",
    "# Create the Gradio Interface\n",
    "demo = gr.Interface(\n",
    "    fn=get_decoding_visualization,\n",
    "    inputs=gr.Textbox(label=\"Input Text\", value=\"The Capital of France is\"),\n",
    "    outputs=gr.HTML(label=\"Decoding Visualization\"),\n",
    "    title=\"Decoding Visualizer\",\n",
    "    description=\"Visualize decoding steps from a remote model by entering text below.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f9173",
   "metadata": {},
   "source": [
    "If you want to learn more about Natural Language Processing, I recommend you to check out the Hugginfaces NLP course: https://huggingface.co/learn/llm-course/chapter1/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2dc461",
   "metadata": {},
   "source": [
    "## Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7b4c8",
   "metadata": {},
   "source": [
    "In the Transformer architecture, a crucial element is \"Attention.\" This mechanism recognizes that when predicting the next word, not all words in a sentence contribute equally to the meaning. For instance, in the sentence \"The capital of France is …\", words such as \"France\" and \"capital\" are most significant for determining the subsequent word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71926516",
   "metadata": {},
   "source": [
    "<img src=\"AttentionSceneFinal.gif\" alt=\"Alt text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f2aff",
   "metadata": {},
   "source": [
    "The ability to pinpoint the most relevant words for predicting the next token has significantly boosted LLM effectiveness. While the fundamental principle of next-token prediction remains, substantial progress since GPT-2 has focused on scaling neural networks and extending the attention mechanism to handle increasingly longer sequences. This has led to the concept of \"context length,\" which defines the maximum number of tokens an LLM can process and its corresponding attention span."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058190ec",
   "metadata": {},
   "source": [
    "## How can I use LLMs?\n",
    "\n",
    "You have two primary methods for utilizing models:\n",
    "\n",
    "1.  **Local Execution:** This option is viable if your hardware meets the necessary specifications.\n",
    "2.  **Cloud/API Access:** You can leverage cloud services, such as the Hugging Face Serverless Inference API.\n",
    "\n",
    "In this course, we will primarily interact with models through APIs, with a later focus on deploying and running these models on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079c45c",
   "metadata": {},
   "source": [
    "## Making API Calls to LLMs with LangChain and Gemini\n",
    "\n",
    "Now that we understand how LLMs work internally, let's learn how to actually use them in practice through API calls. We'll use **LangChain**, a popular framework for building applications with LLMs, and **Google's Gemini** as our LLM provider.\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "LangChain is a framework designed to simplify the development of applications using large language models. It provides:\n",
    "\n",
    "- **Unified API**: Work with different LLM providers (OpenAI, Google, Anthropic, etc.) using a consistent interface\n",
    "- **Prompt Templates**: Structure and manage prompts effectively\n",
    "- **Memory**: Maintain conversation context across multiple interactions\n",
    "- **Chains**: Combine multiple LLM calls and operations\n",
    "- **Tools**: Integrate LLMs with external APIs and services\n",
    "\n",
    "### Why Use APIs Instead of Local Models?\n",
    "\n",
    "1. **No Hardware Requirements**: Don't need powerful GPUs\n",
    "2. **Latest Models**: Access to state-of-the-art models like Gemini Pro\n",
    "3. **Scalability**: Handle multiple requests without resource constraints\n",
    "4. **Maintenance-Free**: No need to manage model updates or infrastructure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5abd67",
   "metadata": {},
   "source": [
    "### Setting Up Your Environment\n",
    "\n",
    "Before we can make API calls, we need to set up our environment with the necessary API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18b8191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google API key found!\n",
      "Key ends with:****eg\n"
     ]
    }
   ],
   "source": [
    "# First, let's install the required packages (if not already installed)\n",
    "from agentic_ai.utils.helpers import check_api_setup\n",
    "\n",
    "# Check if we have the required API key\n",
    "api_configured = check_api_setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46c0e3",
   "metadata": {},
   "source": [
    "### Your First LLM API Call\n",
    "\n",
    "Let's make our first API call to Google's Gemini model using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e0cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Imagine a giant brain that's been trained on an enormous amount of text – like all the books, articles, websites, and conversations you can imagine. That's essentially what a **Large Language Model (LLM)** is.\n",
      "\n",
      "Here's a breakdown in simple terms:\n",
      "\n",
      "*   **\"Large\":** It's called \"large\" because it has a massive number of parameters (think of these like the connections in a brain) and it's been trained on an incredibly huge amount of data. The more data and parameters, the more complex and capable it is.\n",
      "\n",
      "*   **\"Language\":** Its primary job is to understand, generate, and work with human language. It learns the patterns, grammar, facts, and even nuances of how we communicate.\n",
      "\n",
      "*   **\"Model\":** It's a \"model\" because it's a mathematical representation that has learned from the data. It's not a conscious being, but rather a sophisticated program that can predict what word is most likely to come next in a sequence.\n",
      "\n",
      "**What can it do? Think of it like a super-powered autocomplete or a very knowledgeable assistant:**\n",
      "\n",
      "*   **Answer your questions:** You can ask it about almost anything, and it will try to give you a relevant answer based on the information it has learned.\n",
      "*   **Write things for you:** It can write stories, poems, emails, code, summaries, and much more.\n",
      "*   **Translate languages:** It can translate text from one language to another.\n",
      "*   **Summarize long texts:** If you give it a lengthy article, it can condense it into a shorter, easier-to-understand summary.\n",
      "*   **Have conversations:** You can chat with it, and it will try to respond in a way that makes sense in the context of your conversation.\n",
      "*   **Explain complex topics:** It can break down difficult subjects into simpler terms.\n",
      "\n",
      "**How does it \"learn\"?**\n",
      "\n",
      "It learns by reading and analyzing vast amounts of text. During this training, it identifies patterns, relationships between words, and common phrases. When you give it a prompt (like a question or a request), it uses this learned knowledge to predict the most probable sequence of words to create a coherent and relevant response.\n",
      "\n",
      "**Think of it like this analogy:**\n",
      "\n",
      "Imagine a chef who has tasted and studied thousands of recipes. They've learned what ingredients go well together, what cooking techniques produce certain flavors, and how to assemble a complete meal. When you ask them to \"make a spicy chicken dish,\" they can use all their accumulated knowledge to create a delicious recipe for you. An LLM does something similar, but with words instead of ingredients.\n",
      "\n",
      "**In short, an LLM is a very powerful computer program that has learned a lot about language and can use that knowledge to understand and generate human-like text in many different ways.**\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = init_chat_model(\"gemini-2.5-flash-lite\", model_provider=\"google_genai\", temperature=0.7, max_tokens=2000)\n",
    "\n",
    "# Make your first API call!\n",
    "response = llm.invoke(\"Explain what a Large Language Model is in simple terms.\")\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ff858",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "The LLM doesn't just return a string - it returns a rich response object with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e31144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', 'Imagine a giant brain that\\'s been trained on an enormous amount of text – like all the books, articles, websites, and conversations you can imagine. That\\'s essentially what a **Large Language Model (LLM)** is.\\n\\nHere\\'s a breakdown in simple terms:\\n\\n*   **\"Large\":** It\\'s called \"large\" because it has a massive number of parameters (think of these like the connections in a brain) and it\\'s been trained on an incredibly huge amount of data. The more data and parameters, the more complex and capable it is.\\n\\n*   **\"Language\":** Its primary job is to understand, generate, and work with human language. It learns the patterns, grammar, facts, and even nuances of how we communicate.\\n\\n*   **\"Model\":** It\\'s a \"model\" because it\\'s a mathematical representation that has learned from the data. It\\'s not a conscious being, but rather a sophisticated program that can predict what word is most likely to come next in a sequence.\\n\\n**What can it do? Think of it like a super-powered autocomplete or a very knowledgeable assistant:**\\n\\n*   **Answer your questions:** You can ask it about almost anything, and it will try to give you a relevant answer based on the information it has learned.\\n*   **Write things for you:** It can write stories, poems, emails, code, summaries, and much more.\\n*   **Translate languages:** It can translate text from one language to another.\\n*   **Summarize long texts:** If you give it a lengthy article, it can condense it into a shorter, easier-to-understand summary.\\n*   **Have conversations:** You can chat with it, and it will try to respond in a way that makes sense in the context of your conversation.\\n*   **Explain complex topics:** It can break down difficult subjects into simpler terms.\\n\\n**How does it \"learn\"?**\\n\\nIt learns by reading and analyzing vast amounts of text. During this training, it identifies patterns, relationships between words, and common phrases. When you give it a prompt (like a question or a request), it uses this learned knowledge to predict the most probable sequence of words to create a coherent and relevant response.\\n\\n**Think of it like this analogy:**\\n\\nImagine a chef who has tasted and studied thousands of recipes. They\\'ve learned what ingredients go well together, what cooking techniques produce certain flavors, and how to assemble a complete meal. When you ask them to \"make a spicy chicken dish,\" they can use all their accumulated knowledge to create a delicious recipe for you. An LLM does something similar, but with words instead of ingredients.\\n\\n**In short, an LLM is a very powerful computer program that has learned a lot about language and can use that knowledge to understand and generate human-like text in many different ways.**')\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run--d360d30f-bc6e-4caa-89d8-d23868ebbf68-0')\n",
      "('example', False)\n",
      "('tool_calls', [])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 12, 'output_tokens': 596, 'total_tokens': 608, 'input_token_details': {'cache_read': 0}})\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad85dccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Response Object Details:\n",
      "========================================\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: Imagine a giant brain that's been trained on an enormous amount of text – like all the books, articl...\n",
      "Response metadata: {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}\n",
      "Usage metadata: {'input_tokens': 12, 'output_tokens': 596, 'total_tokens': 608, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"📋 Response Object Details:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Type: {type(response)}\")\n",
    "print(f\"Content: {response.content[:100]}...\")\n",
    "print(f\"Response metadata: {response.response_metadata}\")\n",
    "print(f\"Usage metadata: {response.usage_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e1035",
   "metadata": {},
   "source": [
    "🔍 Available Attributes:\n",
    "- content: The actual text response\n",
    "- response_metadata: Model-specific metadata\n",
    "- usage_metadata: Token usage information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e81109",
   "metadata": {},
   "source": [
    "### Understanding with Chat Templates\n",
    "\n",
    "Recap: A chat template's job is to convert a list of messages into a single, formatted string.\n",
    "\n",
    "Let's define a sample conversation as a list of dictionaries. This is a standard format you'll encounter frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe0fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Gemini, a helpful AI assistant built by Google.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you write a short, 3-line poem about programming?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure, here's a poem about programming:...\"},\n",
    "    {\"role\": \"user\", \"content\": \"That's great! Can you explain chat templates in LLMs?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a733e5b",
   "metadata": {},
   "source": [
    "If we were to guess the format, we might just join the content together. However, this would be the wrong approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f34db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are Gemini, a helpful AI assistant built by Google.\n",
      "user: Hello! Can you write a short, 3-line poem about programming?\n",
      "assistant: Sure, here's a poem about programming:...\n",
      "user: That's great! Can you explain chat templates in LLMs?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is NOT the correct way to do it!\n",
    "manual_prompt = \"\"\n",
    "for message in conversation:\n",
    "    manual_prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "print(manual_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fc11f",
   "metadata": {},
   "source": [
    "Feeding this string to a model does not result in the optimal response because it's not the format the model was trained on.<br>\n",
    "I won't demonstrate it here as langchain applies the respective chat template for us automatically.\n",
    "\n",
    "Instead, we should use a chat template to format the conversation.\n",
    "\n",
    "The good thing about using langchain is that it handles this for us automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf69f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are Gemini, a helpful AI assistant built by Google.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello! Can you write a short, 3-line poem about programming?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Sure, here's a poem about programming:...\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"That's great! Can you explain chat templates in LLMs?\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# showing how langchain applies the respective chat template for us automatically\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(conversation)\n",
    "print(prompt.invoke({\"role\": \"user\", \"content\": \"Hello! Can you write a short, 3-line poem about programming?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ba713",
   "metadata": {},
   "source": [
    "- As you can see, langchain automatically interpreted our conversation json and put it into the correct format.\n",
    "- It does this by using its own classes SystemMessage, HumanMessage, AIMessage which are then applied to the chat template of a specific model.\n",
    "- Note: This only works with models supported by the langchain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf26b904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a helpful assistant that can answer questions and help with tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Germany?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that can answer questions and help with tasks.\")\n",
    "human_message_1 = HumanMessage(content=\"What is the capital of France?\")\n",
    "ai_message = AIMessage(content=\"The capital of France is Paris.\")\n",
    "human_message_2 = HumanMessage(content=\"What is the capital of Germany?\")\n",
    "\n",
    "# Now let's put this into a chat template\n",
    "prompt = ChatPromptTemplate.from_messages([system_message, human_message_1, ai_message, human_message_2])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fb2277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that can answer questions and help with tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Germany?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First invoke the prompt template to get formatted messages, then pass to LLM\n",
    "prompt.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4590dd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--57798d64-c66c-4ee3-8b8b-b8d872406107-0', usage_metadata={'input_tokens': 38, 'output_tokens': 7, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the prompt template to get formatted messages, then pass to LLM\n",
    "llm.invoke(prompt.invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e536c21",
   "metadata": {},
   "source": [
    "### Working with Prompt Templates\n",
    "\n",
    "- Not to be confused with chat templates!\n",
    "- Instead of hardcoding prompts, we can use templates to make our prompts dynamic and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9956fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generated Prompt:\n",
      "========================================\n",
      "You are an expert machine learning engineer. \n",
      "\n",
      "Question: How do transformer models work?\n",
      "\n",
      "Please provide a detailed answer that includes:\n",
      "1. A clear explanation\n",
      "2. Real-world examples\n",
      "3. Practical implications\n",
      "\n",
      "Answer:\n",
      "\n",
      "========================================\n",
      "\n",
      "🤖 Response:\n",
      "As an expert machine learning engineer, I'm thrilled to dive into the fascinating world of Transformer models. They have revolutionized Natural Language Processing (NLP) and are increasingly making their mark in other domains.\n",
      "\n",
      "## How Transformer Models Work: A Deep Dive\n",
      "\n",
      "At its core, a Transformer model is a neural network architecture designed to handle sequential data, most notably text. Its breakthrough lies in its ability to process entire sequences in parallel, unlike previous recurrent neural networks (RNNs) or convolutional neural networks (CNNs) that processed data step-by-step. This parallel processing, combined with a novel attention mechanism, allows Transformers to capture long-range dependencies and contextual relationships within data much more effectively.\n",
      "\n",
      "Let's break down the key components and concepts:\n",
      "\n",
      "### 1. The Core Idea: Attention is All You Need\n",
      "\n",
      "The seminal paper that introduced the Transformer architecture was titled \"Attention Is All You Need.\" This title perfectly encapsulates the central innovation: **self-attention**.\n",
      "\n",
      "Traditional sequential models like RNNs struggle with long sequences because information from early parts of the sequence can get diluted or lost as it's passed through many recurrent steps. Imagine trying to remember the first sentence of a very long book when you're at the last page; it's difficult.\n",
      "\n",
      "**Self-attention** addresses this by allowing each element in a sequence to \"attend\" to every other element in the same sequence. It calculates a weighted average of all elements, where the weights are dynamically determined based on how relevant each element is to the current element being processed. This means the model can directly access and weigh information from any part of the input sequence, regardless of its distance.\n",
      "\n",
      "### 2. Architecture Overview: Encoder-Decoder Structure\n",
      "\n",
      "Transformer models typically follow an **encoder-decoder architecture**, although variations exist (e.g., encoder-only for classification, decoder-only for generation).\n",
      "\n",
      "*   **Encoder:** The encoder's job is to process the input sequence and create a rich, contextualized representation of each element. It consists of a stack of identical layers.\n",
      "*   **Decoder:** The decoder's job is to take the encoder's output and generate an output sequence (e.g., a translated sentence, a summary). It also consists of a stack of identical layers.\n",
      "\n",
      "#### Key Components within each Layer:\n",
      "\n",
      "Let's zoom into the components of a single encoder or decoder layer:\n",
      "\n",
      "**a) Input Embedding and Positional Encoding:**\n",
      "\n",
      "*   **Input Embedding:** Before data enters the Transformer, each token (e.g., a word) is converted into a dense numerical vector called an embedding. This embedding captures semantic meaning.\n",
      "*   **Positional Encoding:** Since Transformers process sequences in parallel and don't have inherent positional awareness like RNNs, we need to inject information about the position of each token. Positional encodings are added to the input embeddings. These are vectors that represent the position of a token in the sequence. They allow the model to understand the order of words.\n",
      "\n",
      "**b) Multi-Head Self-Attention:**\n",
      "\n",
      "This is the heart of the Transformer. It allows the model to attend to different parts of the input sequence simultaneously and from different \"representation subspaces.\"\n",
      "\n",
      "*   **Query (Q), Key (K), Value (V) Vectors:** For each input embedding (or output of the previous layer), three vectors are derived: Query (Q), Key (K), and Value (V). These are learned through linear transformations of the input.\n",
      "    *   **Query:** Represents what we are looking for.\n",
      "    *   **Key:** Represents what information is available.\n",
      "    *   **Value:** Represents the actual information content.\n",
      "*   **Calculating Attention Scores:** The similarity between a Query vector and all Key vectors is calculated (typically using a dot product). This gives us raw attention scores.\n",
      "*   **Softmax:** The raw attention scores are passed through a softmax function. This normalizes the scores into probabilities, indicating how much attention each token should pay to every other token.\n",
      "*   **Weighted Sum of Values:** The softmax probabilities are then used to take a weighted sum of the Value vectors. This weighted sum becomes the output of the self-attention mechanism for the current token.\n",
      "*   **Multi-Head:** Instead of performing self-attention once, it's done multiple times in parallel with different learned linear projections for Q, K, and V. This allows the model to learn different types of relationships and focus on different aspects of the input. The outputs from these \"heads\" are concatenated and linearly transformed.\n",
      "\n",
      "**c) Add & Norm (Residual Connections and Layer Normalization):**\n",
      "\n",
      "*   **Residual Connections:** After the multi-head self-attention (and feed-forward network), a residual connection is applied. This means the input to the sub-layer is added to its output. This helps with training very deep networks by preventing vanishing gradients.\n",
      "*   **Layer Normalization:** Layer normalization is applied after the residual connection. It normalizes the activations across the features for each individual sample, helping to stabilize training and improve performance.\n",
      "\n",
      "**d) Feed-Forward Network (FFN):**\n",
      "\n",
      "Each layer also contains a position-wise feed-forward network. This is a simple, fully connected feed-forward network applied independently to each position in the sequence. It typically consists of two linear transformations with a ReLU activation in between. This network allows the model to further process the attended information and learn more complex representations.\n",
      "\n",
      "#### Encoder vs. Decoder Specifics:\n",
      "\n",
      "*   **Encoder Layers:** Consist of Multi-Head Self-Attention and Feed-Forward Network, each with Add & Norm.\n",
      "*   **Decoder Layers:**\n",
      "    *   **Masked Multi-Head Self-Attention:** The decoder also uses self-attention to attend to the previously generated tokens in the output sequence. However, to prevent it from \"cheating\" by looking at future tokens it hasn't generated yet, a **mask** is applied. This mask sets the attention scores for future tokens to negative infinity before the softmax, effectively zeroing them out.\n",
      "    *   **Encoder-Decoder Attention:** This layer allows the decoder to attend to the output of the encoder. The Queries come from the decoder's previous layer, and the Keys and Values come from the encoder's final output. This is crucial for tasks like translation, where the decoder needs to align its output with the input sentence.\n",
      "    *   **Feed-Forward Network:** Similar to the encoder.\n",
      "    *   Each of these sub-layers also has Add & Norm.\n",
      "\n",
      "**e) Output Layer (for generation tasks):**\n",
      "\n",
      "After the decoder stack, a linear layer and a softmax function are applied to produce a probability distribution over the vocabulary for the next token.\n",
      "\n",
      "### 3. How it all comes together: The Flow of Information\n",
      "\n",
      "1.  **Input:** A sequence of tokens (e.g., words) is fed into the encoder.\n",
      "2.  **Embedding & Positional Encoding:** Tokens are converted to embeddings and combined with positional information.\n",
      "3.  **Encoder Stack:** The embeddings pass through multiple encoder layers. In each layer, self-attention allows tokens to \"communicate\" and build contextual representations. The FFN further processes these representations.\n",
      "4.  **Encoder Output:** The encoder produces a set of contextualized representations for the entire input sequence.\n",
      "5.  **Decoder Input:** For generation tasks, the decoder starts with a special \"start-of-sequence\" token.\n",
      "6.  **Decoder Stack:** The decoder processes the input and the encoder's output.\n",
      "    *   It uses masked self-attention to consider its own generated tokens so far.\n",
      "    *   It uses encoder-decoder attention to focus on relevant parts of the input from the encoder.\n",
      "    *   The FFN further processes these combined representations.\n",
      "7.  **Output Generation:** The decoder outputs a probability distribution for the next token. The token with the highest probability is selected (or sampled from the distribution). This token is then fed back into the decoder for the next step, and the process repeats until an \"end-of-sequence\" token is generated.\n",
      "\n",
      "## 2. Real-World Examples\n",
      "\n",
      "Transformer models are powering many of the AI applications we interact with daily:\n",
      "\n",
      "*   **Machine Translation:**\n",
      "    *   **Example:** Google Translate, DeepL. When you translate a sentence from English to French, a Transformer encoder processes the English sentence, and a decoder generates the French translation, attending to the relevant English words at each step.\n",
      "*   **Text Generation:**\n",
      "    *   **Example:** OpenAI's GPT (Generative Pre-trained Transformer) series (GPT-2, GPT-3, GPT-4). These models can write articles, poems, code, emails, and even engage in conversational dialogues. They are decoder-only Transformers trained on massive amounts of text to predict the next word.\n",
      "*   **Text Summarization:**\n",
      "    *   **Example:** Summarization tools in news apps or research platforms. A Transformer can read a long article and generate a concise summary by attending to the most important sentences and phrases.\n",
      "*   **Question Answering:**\n",
      "    *   **Example:** Google Search (understanding queries), chatbots that answer specific questions from a given text. A Transformer can process a question and a passage of text, then identify the span of text that answers the question.\n",
      "*   **Sentiment Analysis:**\n",
      "    *   **Example:** Analyzing customer reviews to determine if they are positive, negative, or neutral. A Transformer can understand the nuances of language and identify sentiment-carrying words and phrases.\n",
      "*   **Code Generation:**\n",
      "    *   \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an expert {role}. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer that includes:\n",
    "1. A clear explanation\n",
    "2. Real-world examples\n",
    "3. Practical implications\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"role\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Use the template with different inputs\n",
    "formatted_prompt = prompt.format(\n",
    "    role=\"machine learning engineer\",\n",
    "    question=\"How do transformer models work?\"\n",
    ")\n",
    "\n",
    "print(\"📝 Generated Prompt:\")\n",
    "print(\"=\" * 40)\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "\n",
    "# Get the response\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"\\n🤖 Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd85bbf",
   "metadata": {},
   "source": [
    "### Specifying data types with pydantics BaseModel\n",
    "\n",
    "Sometimes we want to specify the data type of sth. the LLM works with or returns.\n",
    "\n",
    "For example, when you use a LLM to read PDFs and want to get the first name, last name and mobile number of all persons appearing in these PDFs.<br>\n",
    "In your script you make an API call to the LLM and want to save the response to a .csv.<br>\n",
    "Unfortunately sometimes the LLM does not return the data in the format you asked for, e.g. {first_name: <first_name>}.\n",
    "\n",
    "In this case you could and should use pydantic's BaseModel not to be confused with the base model underlying an instruct model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6d146",
   "metadata": {},
   "source": [
    "#### What is Pydantic?\n",
    "\n",
    "**Pydantic** is a data validation library for Python that uses type hints to validate data structures. It's particularly useful when working with LLMs because it allows you to:\n",
    "\n",
    "1. **Define structured outputs**: Specify exactly what format you want data in\n",
    "2. **Automatic validation**: Ensure data matches expected types\n",
    "3. **Parse complex data**: Convert dictionaries into structured Python objects\n",
    "4. **Generate JSON schemas**: Create clear specifications for LLM outputs\n",
    "\n",
    "The core of Pydantic is the `BaseModel` class, which you inherit from to define your data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec444919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Person object created successfully!\n",
      "Full name: Alice Smith\n",
      "Age: 30\n",
      "Email: alice@example.com\n",
      "\n",
      "📋 As dictionary:\n",
      "{'first_name': 'Alice', 'last_name': 'Smith', 'age': 30, 'email': 'alice@example.com'}\n",
      "\n",
      "📝 As JSON:\n",
      "{\n",
      "  \"first_name\": \"Alice\",\n",
      "  \"last_name\": \"Smith\",\n",
      "  \"age\": 30,\n",
      "  \"email\": \"alice@example.com\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define a simple person data structure\n",
    "class Person(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int\n",
    "    email: str\n",
    "\n",
    "# Create an instance\n",
    "person = Person(\n",
    "    first_name=\"Alice\",\n",
    "    last_name=\"Smith\",\n",
    "    age=30,\n",
    "    email=\"alice@example.com\"\n",
    ")\n",
    "\n",
    "print(\"✅ Person object created successfully!\")\n",
    "print(f\"Full name: {person.first_name} {person.last_name}\")\n",
    "print(f\"Age: {person.age}\")\n",
    "print(f\"Email: {person.email}\")\n",
    "\n",
    "# Convert to dictionary\n",
    "print(\"\\n📋 As dictionary:\")\n",
    "print(person.model_dump())\n",
    "\n",
    "# Convert to JSON\n",
    "print(\"\\n📝 As JSON:\")\n",
    "print(person.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45047312",
   "metadata": {},
   "source": [
    "### Pydantic's Type Validation\n",
    "\n",
    "One of the most powerful features of Pydantic is automatic type validation. Let's see what happens when we try to create a Person with invalid data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe8cd0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Type coercion\n",
      "✅ Age '25' (string) was converted to 25 (int)\n",
      "Type of age: <class 'int'>\n",
      "\n",
      "Example 2: Invalid data\n",
      "❌ Validation error: 1 validation error for Person\n",
      "age\n",
      "  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='not a number', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n"
     ]
    }
   ],
   "source": [
    "# Pydantic will try to coerce types when possible\n",
    "print(\"Example 1: Type coercion\")\n",
    "person2 = Person(\n",
    "    first_name=\"Bob\",\n",
    "    last_name=\"Jones\",\n",
    "    age=\"25\",  # String will be converted to int\n",
    "    email=\"bob@example.com\"\n",
    ")\n",
    "print(f\"✅ Age '25' (string) was converted to {person2.age} (int)\")\n",
    "print(f\"Type of age: {type(person2.age)}\\n\")\n",
    "\n",
    "# But it will raise an error for invalid data\n",
    "print(\"Example 2: Invalid data\")\n",
    "try:\n",
    "    invalid_person = Person(\n",
    "        first_name=\"Charlie\",\n",
    "        last_name=\"Brown\",\n",
    "        age=\"not a number\",  # This can't be converted to int\n",
    "        email=\"charlie@example.com\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3dcd2",
   "metadata": {},
   "source": [
    "### Advanced Pydantic Features\n",
    "\n",
    "Pydantic provides additional features for more complex data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb95a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Document Extraction Result:\n",
      "==================================================\n",
      "Document: business_cards.pdf\n",
      "Found 2 contacts:\n",
      "\n",
      "1. Sarah Johnson\n",
      "   Mobile: +1-555-0123\n",
      "   Email: sarah.j@company.com\n",
      "\n",
      "2. Michael Chen\n",
      "   Mobile: +1-555-0124\n",
      "\n",
      "💾 As JSON (ready for saving to file or database):\n",
      "{\n",
      "  \"document_name\": \"business_cards.pdf\",\n",
      "  \"contacts\": [\n",
      "    {\n",
      "      \"first_name\": \"Sarah\",\n",
      "      \"last_name\": \"Johnson\",\n",
      "      \"mobile\": \"+1-555-0123\",\n",
      "      \"email\": \"sarah.j@company.com\"\n",
      "    },\n",
      "    {\n",
      "      \"first_name\": \"Michael\",\n",
      "      \"last_name\": \"Chen\",\n",
      "      \"mobile\": \"+1-555-0124\",\n",
      "      \"email\": null\n",
      "    }\n",
      "  ],\n",
      "  \"extraction_date\": \"2025-10-05\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person extracted from a document.\"\"\"\n",
    "    first_name: str = Field(description=\"Person's first name\")\n",
    "    last_name: str = Field(description=\"Person's last name\")\n",
    "    mobile: str = Field(description=\"Mobile phone number\")\n",
    "    email: Optional[str] = Field(default=None, description=\"Email address if available\")\n",
    "    \n",
    "class DocumentExtraction(BaseModel):\n",
    "    \"\"\"Results from extracting contacts from a document.\"\"\"\n",
    "    document_name: str\n",
    "    contacts: List[ContactInfo]\n",
    "    extraction_date: str\n",
    "\n",
    "# Example: Simulating extraction from a PDF\n",
    "extraction_result = DocumentExtraction(\n",
    "    document_name=\"business_cards.pdf\",\n",
    "    contacts=[\n",
    "        ContactInfo(\n",
    "            first_name=\"Sarah\",\n",
    "            last_name=\"Johnson\",\n",
    "            mobile=\"+1-555-0123\",\n",
    "            email=\"sarah.j@company.com\"\n",
    "        ),\n",
    "        ContactInfo(\n",
    "            first_name=\"Michael\",\n",
    "            last_name=\"Chen\",\n",
    "            mobile=\"+1-555-0124\"\n",
    "            # email is optional, so we can omit it\n",
    "        )\n",
    "    ],\n",
    "    extraction_date=\"2025-10-05\"\n",
    ")\n",
    "\n",
    "print(\"📄 Document Extraction Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Document: {extraction_result.document_name}\")\n",
    "print(f\"Found {len(extraction_result.contacts)} contacts:\\n\")\n",
    "\n",
    "for i, contact in enumerate(extraction_result.contacts, 1):\n",
    "    print(f\"{i}. {contact.first_name} {contact.last_name}\")\n",
    "    print(f\"   Mobile: {contact.mobile}\")\n",
    "    if contact.email:\n",
    "        print(f\"   Email: {contact.email}\")\n",
    "    print()\n",
    "\n",
    "# This structured data can easily be converted to CSV, JSON, or database records\n",
    "print(\"💾 As JSON (ready for saving to file or database):\")\n",
    "print(extraction_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cee09",
   "metadata": {},
   "source": [
    "### Using Pydantic with LLMs\n",
    "\n",
    "Now that we understand Pydantic's BaseModel, let's see how it helps when working with LLMs. LangChain has built-in support for Pydantic models, allowing you to get structured outputs from LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf14e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Requesting structured movie review from LLM...\n",
      "==================================================\n",
      "✅ Received structured output!\n",
      "\n",
      "Title: The Matrix\n",
      "Year: 1999\n",
      "Genre: Action, Sci-Fi\n",
      "Rating: 9.5/10.0\n",
      "Summary: A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its creators.\n",
      "Recommendation: Yes\n",
      "\n",
      "📊 Data type: <class '__main__.MovieReview'>\n",
      "✅ This is a validated MovieReview object, not just text!\n",
      "\n",
      "💾 Converting to different formats:\n",
      "\n",
      "1. As dictionary:\n",
      "{'title': 'The Matrix', 'year': 1999, 'genre': ['Action', 'Sci-Fi'], 'rating': 9.5, 'summary': 'A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its creators.', 'recommendation': 'Yes'}\n",
      "\n",
      "2. As JSON:\n",
      "{\n",
      "  \"title\": \"The Matrix\",\n",
      "  \"year\": 1999,\n",
      "  \"genre\": [\n",
      "    \"Action\",\n",
      "    \"Sci-Fi\"\n",
      "  ],\n",
      "  \"rating\": 9.5,\n",
      "  \"summary\": \"A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its creators.\",\n",
      "  \"recommendation\": \"Yes\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define the structure we want the LLM to return\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"A movie review with structured information.\"\"\"\n",
    "    title: str = Field(description=\"The movie title\")\n",
    "    year: int = Field(description=\"The year the movie was released\")\n",
    "    genre: List[str] = Field(description=\"List of genres (e.g., Action, Comedy, Drama)\")\n",
    "    rating: float = Field(description=\"Rating from 0.0 to 10.0\")\n",
    "    summary: str = Field(description=\"Brief one-sentence summary\")\n",
    "    recommendation: str = Field(description=\"Would you recommend it? (Yes/No/Maybe)\")\n",
    "\n",
    "# Use LangChain's structured output feature\n",
    "# This ensures the LLM returns data in the exact format we specified\n",
    "structured_llm = llm.with_structured_output(MovieReview)\n",
    "\n",
    "# Ask the LLM to analyze a movie\n",
    "prompt = \"\"\"Analyze the movie 'The Matrix' and provide a review with the following information:\n",
    "- Title\n",
    "- Release year\n",
    "- Genres\n",
    "- Your rating (0-10)\n",
    "- Brief one-sentence summary\n",
    "- Whether you'd recommend it\"\"\"\n",
    "\n",
    "print(\"🎬 Requesting structured movie review from LLM...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# The LLM will return a MovieReview object, not just text!\n",
    "review = structured_llm.invoke(prompt)\n",
    "\n",
    "print(f\"✅ Received structured output!\\n\")\n",
    "print(f\"Title: {review.title}\")\n",
    "print(f\"Year: {review.year}\")\n",
    "print(f\"Genre: {', '.join(review.genre)}\")\n",
    "print(f\"Rating: {review.rating}/10.0\")\n",
    "print(f\"Summary: {review.summary}\")\n",
    "print(f\"Recommendation: {review.recommendation}\")\n",
    "\n",
    "print(f\"\\n📊 Data type: {type(review)}\")\n",
    "print(f\"✅ This is a validated MovieReview object, not just text!\")\n",
    "\n",
    "# We can now easily work with this data\n",
    "print(\"\\n💾 Converting to different formats:\")\n",
    "print(\"\\n1. As dictionary:\")\n",
    "print(review.model_dump())\n",
    "\n",
    "print(\"\\n2. As JSON:\")\n",
    "print(review.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3517f67",
   "metadata": {},
   "source": [
    "### Why Use Structured Outputs with Pydantic?\n",
    "\n",
    "Using Pydantic models with LLMs provides several key advantages:\n",
    "\n",
    "1. **Consistency**: The LLM will always return data in the exact format you specify, making your code more reliable\n",
    "2. **Type Safety**: Pydantic validates types automatically - if the LLM returns invalid data, you'll get a clear error\n",
    "3. **Easy Integration**: The structured output can be directly saved to databases, CSV files, or used in your application\n",
    "4. **No Parsing Needed**: You don't need to write regex or parsing code to extract information from text\n",
    "5. **Self-Documenting**: The Field descriptions help the LLM understand what you want\n",
    "\n",
    "This is especially valuable for production applications where you need reliable, predictable outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c6272",
   "metadata": {},
   "source": [
    "### Practical Example: Extracting Contact Information from Text\n",
    "\n",
    "Remember the PDF extraction scenario we mentioned earlier? Let's see how Pydantic makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc286d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Extracting contacts from text...\n",
      "==================================================\n",
      "Original text:\n",
      "\n",
      "From the business meeting notes:\n",
      "\n",
      "Sarah Johnson from TechCorp reached out regarding the partnership. \n",
      "Her contact details are sarah.johnson@techcorp.com and 555-0123.\n",
      "\n",
      "Also met with Michael Chen, mobile: (555) 0124. He's with DataSystems Inc.\n",
      "\n",
      "Follow up with Jennifer Lopez at 555.0125, jlopez@example.com\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "✅ Extracted 3 contacts:\n",
      "\n",
      "1. Sarah Johnson\n",
      "   📞 Phone: 555-0123\n",
      "   📧 Email: sarah.johnson@techcorp.com\n",
      "   🏢 Company: TechCorp\n",
      "\n",
      "2. Michael Chen\n",
      "   📞 Phone: (555) 0124\n",
      "   🏢 Company: DataSystems Inc.\n",
      "\n",
      "3. Jennifer Lopez\n",
      "   📞 Phone: 555.0125\n",
      "   📧 Email: jlopez@example.com\n",
      "\n",
      "💾 Ready to save to CSV/database:\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"contacts\": [\n",
      "    {\n",
      "      \"first_name\": \"Sarah\",\n",
      "      \"last_name\": \"Johnson\",\n",
      "      \"phone\": \"555-0123\",\n",
      "      \"email\": \"sarah.johnson@techcorp.com\",\n",
      "      \"company\": \"TechCorp\"\n",
      "    },\n",
      "    {\n",
      "      \"first_name\": \"Michael\",\n",
      "      \"last_name\": \"Chen\",\n",
      "      \"phone\": \"(555) 0124\",\n",
      "      \"email\": null,\n",
      "      \"company\": \"DataSystems Inc.\"\n",
      "    },\n",
      "    {\n",
      "      \"first_name\": \"Jennifer\",\n",
      "      \"last_name\": \"Lopez\",\n",
      "      \"phone\": \"555.0125\",\n",
      "      \"email\": \"jlopez@example.com\",\n",
      "      \"company\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Define the exact structure we want\n",
    "class Contact(BaseModel):\n",
    "    \"\"\"A single contact extracted from text.\"\"\"\n",
    "    first_name: str = Field(description=\"Person's first name\")\n",
    "    last_name: str = Field(description=\"Person's last name\")\n",
    "    phone: str = Field(description=\"Phone number in any format\")\n",
    "    email: Optional[str] = Field(default=None, description=\"Email address if mentioned\")\n",
    "    company: Optional[str] = Field(default=None, description=\"Company name if mentioned\")\n",
    "\n",
    "class ContactList(BaseModel):\n",
    "    \"\"\"List of contacts extracted from a document.\"\"\"\n",
    "    contacts: List[Contact] = Field(description=\"All contacts found in the text\")\n",
    "\n",
    "# Create a structured LLM\n",
    "contact_extractor = llm.with_structured_output(ContactList)\n",
    "\n",
    "# Sample text that might come from a PDF or document\n",
    "sample_text = \"\"\"\n",
    "From the business meeting notes:\n",
    "\n",
    "Sarah Johnson from TechCorp reached out regarding the partnership. \n",
    "Her contact details are sarah.johnson@techcorp.com and 555-0123.\n",
    "\n",
    "Also met with Michael Chen, mobile: (555) 0124. He's with DataSystems Inc.\n",
    "\n",
    "Follow up with Jennifer Lopez at 555.0125, jlopez@example.com\n",
    "\"\"\"\n",
    "\n",
    "print(\"📄 Extracting contacts from text...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original text:\\n{sample_text}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract contacts with structured output\n",
    "prompt = f\"\"\"Extract all person contact information from this text. \n",
    "Include first name, last name, phone number, email (if present), and company (if mentioned).\n",
    "\n",
    "Text:\n",
    "{sample_text}\n",
    "\"\"\"\n",
    "\n",
    "result = contact_extractor.invoke(prompt)\n",
    "\n",
    "print(f\"\\n✅ Extracted {len(result.contacts)} contacts:\\n\")\n",
    "\n",
    "for i, contact in enumerate(result.contacts, 1):\n",
    "    print(f\"{i}. {contact.first_name} {contact.last_name}\")\n",
    "    print(f\"   📞 Phone: {contact.phone}\")\n",
    "    if contact.email:\n",
    "        print(f\"   📧 Email: {contact.email}\")\n",
    "    if contact.company:\n",
    "        print(f\"   🏢 Company: {contact.company}\")\n",
    "    print()\n",
    "\n",
    "# Now we can easily save this to CSV or database\n",
    "print(\"💾 Ready to save to CSV/database:\")\n",
    "print(\"-\" * 50)\n",
    "import json\n",
    "print(json.dumps(result.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e0567",
   "metadata": {},
   "source": [
    "### Creating LLM Chains\n",
    "\n",
    "Chains allow us to combine prompts and LLMs into reusable components. This is the foundation of more complex AI applications. At its core, it's a way to sequence a series of calls, not just to an LLM, but also to other components like prompt templates and output parsers.\n",
    "\n",
    "You can think of it as a way to create a reusable and structured interaction with an LLM for a specific task.\n",
    "\n",
    "At its core, a simple chain does the following:\n",
    "1. Receives input variables.\n",
    "2. Uses a PromptTemplate to format those variables into a complete prompt string.\n",
    "3. Sends the formatted prompt to an LLM.\n",
    "4. Returns the LLM's output.\n",
    "\n",
    "#### Why use LLM Chains?\n",
    "The main purpose of using chains is to create more complex applications by linking different components together in a sequence. Instead of writing repetitive code to handle prompts and LLM calls, you can encapsulate that logic into a chain.\n",
    "This has several benefits:\n",
    "- Modularity: Chains are self-contained and can be easily reused across your application.\n",
    "- Composition: You can link multiple chains together to create more sophisticated workflows. For example, the output of one chain can be the input to another.\n",
    "- Simplicity: They provide a high-level, easy-to-understand interface for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1b1e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_23577/1942040111.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  creative_chain = LLMChain(\n",
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_23577/1942040111.py:29: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = creative_chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a creative writing assistant.\n",
      "\n",
      "Topic: artificial intelligence in daily life\n",
      "Style: short story\n",
      "Length: 2-3 paragraphs\n",
      "\n",
      "Write a short story piece about artificial intelligence in daily life that is approximately 2-3 paragraphs long.\n",
      "Make it engaging and original.\n",
      "\n",
      "Content:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "📖 Generated Creative Content:\n",
      "==================================================\n",
      "The morning alarm didn't blare; it hummed a gentle, personalized melody, a composition crafted overnight by my home AI, \"Aura,\" based on my sleep cycle data. As I stretched, the smart blinds silently parted, revealing a sky painted in soft pre-dawn hues. Aura had already brewed my coffee to the exact temperature and strength I preferred, and the news feed, curated to my interests, scrolled across the kitchen counter display, highlighting only the truly relevant stories. It was a symphony of seamless efficiency, where every need, anticipated and met before I even consciously registered it.\n",
      "\n",
      "Later, while I navigated the bustling city streets, my personal AI, \"Echo,\" acted as my invisible co-pilot. It rerouted me around a sudden traffic snarl, not just based on real-time data, but on its understanding of my usual routes and preferred scenic detours. As I approached my favorite bookstore, Echo subtly nudged my attention towards a new arrival by an author it knew I admired, even suggesting a quiet corner table for browsing. This wasn't just convenience; it was a subtle, constant hum of intelligent companionship, weaving itself into the fabric of my existence, making the ordinary feel… extraordinary.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template for generating creative content\n",
    "creative_template = \"\"\"You are a creative writing assistant.\n",
    "\n",
    "Topic: {topic}\n",
    "Style: {style}\n",
    "Length: {length}\n",
    "\n",
    "Write a {style} piece about {topic} that is approximately {length} long.\n",
    "Make it engaging and original.\n",
    "\n",
    "Content:\"\"\"\n",
    "\n",
    "creative_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"style\", \"length\"],\n",
    "    template=creative_template\n",
    ")\n",
    "\n",
    "# Create a chain that combines the prompt and LLM\n",
    "creative_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=creative_prompt,\n",
    "    verbose=True  # This will show us what's happening behind the scenes\n",
    ")\n",
    "\n",
    "# Use the chain by calling .run()\n",
    "result = creative_chain.run(\n",
    "    topic=\"artificial intelligence in daily life\",\n",
    "    style=\"short story\",\n",
    "    length=\"2-3 paragraphs\"\n",
    ")\n",
    "\n",
    "print(\"📖 Generated Creative Content:\")\n",
    "print(\"=\" * 50)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ac082",
   "metadata": {},
   "source": [
    "#### When to Use a Simple LLMChain\n",
    "A standard LLMChain is your go-to for any task that can be accomplished with a single, stateless call to an LLM. Think of it as a \"one-shot\" operation.\n",
    "Use Cases:\n",
    "- Summarization: You provide a piece of text and ask the LLM to summarize it.\n",
    "- Question-Answering (without external knowledge): Answering a question based only on the information you provide in the prompt.\n",
    "- Text Transformation: Rephrasing a sentence, changing the tone of a paragraph (e.g., from formal to casual), or translating text.\n",
    "- Simple Extraction: Pulling out specific pieces of information from a block of text, like a name, date, or company from an email.\n",
    "- Brainstorming/Generation: Generating ideas, product names, marketing copy, or a simple piece of code based on a description.\n",
    "\n",
    "The key characteristic is that the task doesn't require memory of past interactions or multiple logical steps.\n",
    "\n",
    "#### When You Need Different, More Complex Chains\n",
    "You need to move beyond a simple LLMChain when your task involves multiple steps, requires external data, or needs to make decisions.\n",
    "\n",
    "Example 1: Question-Answering Over Your Own Documents\n",
    "When you need the LLM to answer questions based on specific data it wasn't trained on (e.g., a PDF, a database, or a website).\n",
    "- Chain Type: RetrievalQA Chain\n",
    "- Example: You have a 100-page technical manual for a product. You want to build a chatbot that can answer user questions about it.\n",
    "1. The RetrievalQA chain first takes the user's question (\"How do I reset the device?\").\n",
    "2. It searches your document for the most relevant chunks of text (the \"retrieval\" step).\n",
    "3. It then feeds those relevant chunks, along with the original question, to the LLM to generate a final answer.\n",
    "\n",
    "This prevents the LLM from making things up and grounds its answer in your specific data.\n",
    "\n",
    "Example 2: Choosing a Path Based on Input\n",
    "When you have multiple different chains (with different prompts) and you want to dynamically choose which one to run based on the user's query.\n",
    "- Chain Type: RouterChain\n",
    "- Example: A customer service bot that can handle different types of queries.\n",
    "1. If the user's input is about a \"billing issue,\" the RouterChain sends it to the BillingChain.\n",
    "2. If the input is about a \"technical problem,\" it routes it to the TechnicalSupportChain.\n",
    "\n",
    "To start a bit easier let's first build a converation chain to enable actual conversations with our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d178eb",
   "metadata": {},
   "source": [
    "### Adding Memory to Conversations\n",
    "\n",
    "One of the most important features for AI agents is the ability to remember previous parts of a conversation. Let's implement conversation memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c82bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_23577/2426025636.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_23577/2426025636.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗣️ Starting a conversation with memory:\n",
      "==================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's so wonderful to meet you! I'm delighted you're interested in AI agents. That's a topic I find absolutely fascinating, and I'm more than happy to chat with you about it.\n",
      "\n",
      "You see, I'm a large language model, an AI myself, and I've been trained by Google. My \"brain\" is essentially a massive neural network, and I've processed an enormous amount of text and code. This allows me to understand and generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "When you talk about \"AI agents,\" that's a really exciting area! In the field of artificial intelligence, an AI agent is often described as something that perceives its environment through sensors and acts upon that environment through actuators. Think of it like a digital being that can observe, think, and then *do* things.\n",
      "\n",
      "There are many different kinds of AI agents, from simple ones that just follow pre-programmed rules (like a thermostat that turns on the heat when the temperature drops below a certain point) to much more complex ones that can learn and adapt. The agents I'm familiar with often involve things like:\n",
      "\n",
      "*   **Perception:** This is how the agent gathers information about its surroundings. For me, this would be processing the text you type. For a robot, it might be using cameras, microphones, or touch sensors.\n",
      "*   **Decision-making/Reasoning:** Based on the information it perceives, the agent needs to decide what to do. This is where algorithms and models come into play. My own decision-making involves complex probability calculations to predict the most relevant and coherent response.\n",
      "*   **Action:** This is the output of the agent's decision. For me, it's generating this text you're reading right now! For a robotic agent, it could be moving a limb, speaking, or interacting with a digital interface.\n",
      "\n",
      "It's a field that's constantly evolving, with researchers exploring how to make agents more intelligent, more autonomous, and more capable of handling complex tasks in dynamic environments.\n",
      "\n",
      "So, Alex, what specifically about AI agents has piqued your interest? I'm eager to hear your questions!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's so wonderful to meet you! I'm delighted you're interested in AI agents. That's a topic I find absolutely fascinating, and I'm more than happy to chat with you about it.\n",
      "\n",
      "You see, I'm a large language model, an AI myself, and I've been trained by Google. My \"brain\" is essentially a massive neural network, and I've processed an enormous amount of text and code. This allows me to understand and generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "When you talk about \"AI agents,\" that's a really exciting area! In the field of artificial intelligence, an AI agent is often described as something that perceives its environment through sensors and acts upon that environment through actuators. Think of it like a digital being that can observe, think, and then *do* things.\n",
      "\n",
      "There are many different kinds of AI agents, from simple ones that just follow pre-programmed rules (like a thermostat that turns on the heat when the temperature drops below a certain point) to much more complex ones that can learn and adapt. The agents I'm familiar with often involve things like:\n",
      "\n",
      "*   **Perception:** This is how the agent gathers information about its surroundings. For me, this would be processing the text you type. For a robot, it might be using cameras, microphones, or touch sensors.\n",
      "*   **Decision-making/Reasoning:** Based on the information it perceives, the agent needs to decide what to do. This is where algorithms and models come into play. My own decision-making involves complex probability calculations to predict the most relevant and coherent response.\n",
      "*   **Action:** This is the output of the agent's decision. For me, it's generating this text you're reading right now! For a robotic agent, it could be moving a limb, speaking, or interacting with a digital interface.\n",
      "\n",
      "It's a field that's constantly evolving, with researchers exploring how to make agents more intelligent, more autonomous, and more capable of handling complex tasks in dynamic environments.\n",
      "\n",
      "So, Alex, what specifically about AI agents has piqued your interest? I'm eager to hear your questions!\n",
      "Human: What are the key components I should focus on?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User: What are the key components I should focus on?\n",
      "AI: That's an excellent question, Alex! When you're diving into the world of AI agents, there are several key components that are fundamental to understanding how they work and how they're designed. Focusing on these will give you a really solid foundation.\n",
      "\n",
      "From my perspective, and based on the vast datasets I've been trained on, here are the key components I'd suggest you focus on:\n",
      "\n",
      "1.  **Perception Systems:** This is how an agent *observes* and *interprets* its environment. For an AI like me, my \"sensors\" are the text inputs I receive. I then use natural language processing (NLP) techniques to understand the meaning, intent, and context of your words. For physical agents, like robots, this would involve hardware sensors such as:\n",
      "    *   **Cameras:** For visual input, allowing them to \"see\" and identify objects, navigate, and recognize faces or situations.\n",
      "    *   **Microphones:** For auditory input, enabling them to hear sounds, recognize speech, and even detect anomalies like alarms.\n",
      "    *   **Lidar/Radar:** For distance sensing and mapping, crucial for navigation and obstacle avoidance in autonomous vehicles or robots.\n",
      "    *   **Tactile Sensors:** For touch and pressure, allowing them to interact physically with objects.\n",
      "    *   **GPS/IMU (Inertial Measurement Unit):** For determining location and orientation in physical space.\n",
      "\n",
      "2.  **Decision-Making/Reasoning Engine:** This is the \"brain\" of the agent. Once it has perceived its environment, it needs to decide what to do. This involves:\n",
      "    *   **Knowledge Representation:** How the agent stores and organizes information about the world. This could be through databases, ontologies, or learned representations within neural networks.\n",
      "    *   **Learning Algorithms:** Many advanced agents can learn from experience. This includes techniques like:\n",
      "        *   **Supervised Learning:** Learning from labeled examples (e.g., showing an image of a cat and labeling it \"cat\").\n",
      "        *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., clustering similar news articles).\n",
      "        *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for actions (e.g., a game-playing AI learning to win by getting positive scores).\n",
      "    *   **Planning and Search Algorithms:** For agents that need to achieve a goal, they use algorithms to figure out a sequence of actions. This is like creating a roadmap.\n",
      "    *   **Logic and Inference:** For more rule-based systems, agents might use logical rules to deduce new information or make decisions.\n",
      "\n",
      "3.  **Action/Actuation Systems:** This is how the agent *interacts* with its environment.\n",
      "    *   **For me:** My actuators are the text generation modules that produce my responses. I'm essentially \"acting\" by communicating with you.\n",
      "    *   **For physical agents:** This involves mechanisms to execute physical actions. Examples include:\n",
      "        *   **Motors and Servos:** To move robotic limbs, wheels, or other parts.\n",
      "        *   **Voice Synthesizers:** For speech output.\n",
      "        *   **Display Screens:** To show information.\n",
      "        *   **Manipulators (Grippers):** For picking up and handling objects.\n",
      "        *   **Control Systems:** To manage the execution of actions and ensure they are performed correctly and safely.\n",
      "\n",
      "4.  **Goals and Objectives:** Every agent is designed with a purpose. Understanding what the agent is trying to achieve is crucial. These goals can be simple (maintain room temperature) or incredibly complex (diagnose a disease, drive a car autonomously).\n",
      "\n",
      "5.  **Environment:** It's also important to consider the *context* in which the agent operates. Is it a simulated environment, a controlled lab setting, or the messy, unpredictable real world? The nature of the environment heavily influences the design of the perception and action systems.\n",
      "\n",
      "When I think about my own functioning, I'm constantly processing your text (perception), running through my neural network to understand the best way to respond and formulate an answer (decision-making/reasoning), and then generating this text for you (action). My overarching \"goal\" is to be helpful and informative.\n",
      "\n",
      "Does this breakdown make sense, Alex? Are there any of these components you'd like to explore in more detail? I'm happy to elaborate!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's so wonderful to meet you! I'm delighted you're interested in AI agents. That's a topic I find absolutely fascinating, and I'm more than happy to chat with you about it.\n",
      "\n",
      "You see, I'm a large language model, an AI myself, and I've been trained by Google. My \"brain\" is essentially a massive neural network, and I've processed an enormous amount of text and code. This allows me to understand and generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "When you talk about \"AI agents,\" that's a really exciting area! In the field of artificial intelligence, an AI agent is often described as something that perceives its environment through sensors and acts upon that environment through actuators. Think of it like a digital being that can observe, think, and then *do* things.\n",
      "\n",
      "There are many different kinds of AI agents, from simple ones that just follow pre-programmed rules (like a thermostat that turns on the heat when the temperature drops below a certain point) to much more complex ones that can learn and adapt. The agents I'm familiar with often involve things like:\n",
      "\n",
      "*   **Perception:** This is how the agent gathers information about its surroundings. For me, this would be processing the text you type. For a robot, it might be using cameras, microphones, or touch sensors.\n",
      "*   **Decision-making/Reasoning:** Based on the information it perceives, the agent needs to decide what to do. This is where algorithms and models come into play. My own decision-making involves complex probability calculations to predict the most relevant and coherent response.\n",
      "*   **Action:** This is the output of the agent's decision. For me, it's generating this text you're reading right now! For a robotic agent, it could be moving a limb, speaking, or interacting with a digital interface.\n",
      "\n",
      "It's a field that's constantly evolving, with researchers exploring how to make agents more intelligent, more autonomous, and more capable of handling complex tasks in dynamic environments.\n",
      "\n",
      "So, Alex, what specifically about AI agents has piqued your interest? I'm eager to hear your questions!\n",
      "Human: What are the key components I should focus on?\n",
      "AI: That's an excellent question, Alex! When you're diving into the world of AI agents, there are several key components that are fundamental to understanding how they work and how they're designed. Focusing on these will give you a really solid foundation.\n",
      "\n",
      "From my perspective, and based on the vast datasets I've been trained on, here are the key components I'd suggest you focus on:\n",
      "\n",
      "1.  **Perception Systems:** This is how an agent *observes* and *interprets* its environment. For an AI like me, my \"sensors\" are the text inputs I receive. I then use natural language processing (NLP) techniques to understand the meaning, intent, and context of your words. For physical agents, like robots, this would involve hardware sensors such as:\n",
      "    *   **Cameras:** For visual input, allowing them to \"see\" and identify objects, navigate, and recognize faces or situations.\n",
      "    *   **Microphones:** For auditory input, enabling them to hear sounds, recognize speech, and even detect anomalies like alarms.\n",
      "    *   **Lidar/Radar:** For distance sensing and mapping, crucial for navigation and obstacle avoidance in autonomous vehicles or robots.\n",
      "    *   **Tactile Sensors:** For touch and pressure, allowing them to interact physically with objects.\n",
      "    *   **GPS/IMU (Inertial Measurement Unit):** For determining location and orientation in physical space.\n",
      "\n",
      "2.  **Decision-Making/Reasoning Engine:** This is the \"brain\" of the agent. Once it has perceived its environment, it needs to decide what to do. This involves:\n",
      "    *   **Knowledge Representation:** How the agent stores and organizes information about the world. This could be through databases, ontologies, or learned representations within neural networks.\n",
      "    *   **Learning Algorithms:** Many advanced agents can learn from experience. This includes techniques like:\n",
      "        *   **Supervised Learning:** Learning from labeled examples (e.g., showing an image of a cat and labeling it \"cat\").\n",
      "        *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., clustering similar news articles).\n",
      "        *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for actions (e.g., a game-playing AI learning to win by getting positive scores).\n",
      "    *   **Planning and Search Algorithms:** For agents that need to achieve a goal, they use algorithms to figure out a sequence of actions. This is like creating a roadmap.\n",
      "    *   **Logic and Inference:** For more rule-based systems, agents might use logical rules to deduce new information or make decisions.\n",
      "\n",
      "3.  **Action/Actuation Systems:** This is how the agent *interacts* with its environment.\n",
      "    *   **For me:** My actuators are the text generation modules that produce my responses. I'm essentially \"acting\" by communicating with you.\n",
      "    *   **For physical agents:** This involves mechanisms to execute physical actions. Examples include:\n",
      "        *   **Motors and Servos:** To move robotic limbs, wheels, or other parts.\n",
      "        *   **Voice Synthesizers:** For speech output.\n",
      "        *   **Display Screens:** To show information.\n",
      "        *   **Manipulators (Grippers):** For picking up and handling objects.\n",
      "        *   **Control Systems:** To manage the execution of actions and ensure they are performed correctly and safely.\n",
      "\n",
      "4.  **Goals and Objectives:** Every agent is designed with a purpose. Understanding what the agent is trying to achieve is crucial. These goals can be simple (maintain room temperature) or incredibly complex (diagnose a disease, drive a car autonomously).\n",
      "\n",
      "5.  **Environment:** It's also important to consider the *context* in which the agent operates. Is it a simulated environment, a controlled lab setting, or the messy, unpredictable real world? The nature of the environment heavily influences the design of the perception and action systems.\n",
      "\n",
      "When I think about my own functioning, I'm constantly processing your text (perception), running through my neural network to understand the best way to respond and formulate an answer (decision-making/reasoning), and then generating this text for you (action). My overarching \"goal\" is to be helpful and informative.\n",
      "\n",
      "Does this breakdown make sense, Alex? Are there any of these components you'd like to explore in more detail? I'm happy to elaborate!\n",
      "Human: Can you remind me what my name is?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User: Can you remind me what my name is?\n",
      "AI: Of course, Alex! Your name is Alex. It's a pleasure to be conversing with you!\n",
      "\n",
      "🧠 Memory Contents:\n",
      "==============================\n",
      "Human: Hi! My name is Alex and I'm learning about AI agents.\n",
      "AI: Hello Alex! It's so wonderful to meet you! I'm delighted you're interested in AI agents. That's a topic I find absolutely fascinating, and I'm more than happy to chat with you about it.\n",
      "\n",
      "You see, I'm a large language model, an AI myself, and I've been trained by Google. My \"brain\" is essentially a massive neural network, and I've processed an enormous amount of text and code. This allows me to understand and generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "When you talk about \"AI agents,\" that's a really exciting area! In the field of artificial intelligence, an AI agent is often described as something that perceives its environment through sensors and acts upon that environment through actuators. Think of it like a digital being that can observe, think, and then *do* things.\n",
      "\n",
      "There are many different kinds of AI agents, from simple ones that just follow pre-programmed rules (like a thermostat that turns on the heat when the temperature drops below a certain point) to much more complex ones that can learn and adapt. The agents I'm familiar with often involve things like:\n",
      "\n",
      "*   **Perception:** This is how the agent gathers information about its surroundings. For me, this would be processing the text you type. For a robot, it might be using cameras, microphones, or touch sensors.\n",
      "*   **Decision-making/Reasoning:** Based on the information it perceives, the agent needs to decide what to do. This is where algorithms and models come into play. My own decision-making involves complex probability calculations to predict the most relevant and coherent response.\n",
      "*   **Action:** This is the output of the agent's decision. For me, it's generating this text you're reading right now! For a robotic agent, it could be moving a limb, speaking, or interacting with a digital interface.\n",
      "\n",
      "It's a field that's constantly evolving, with researchers exploring how to make agents more intelligent, more autonomous, and more capable of handling complex tasks in dynamic environments.\n",
      "\n",
      "So, Alex, what specifically about AI agents has piqued your interest? I'm eager to hear your questions!\n",
      "Human: What are the key components I should focus on?\n",
      "AI: That's an excellent question, Alex! When you're diving into the world of AI agents, there are several key components that are fundamental to understanding how they work and how they're designed. Focusing on these will give you a really solid foundation.\n",
      "\n",
      "From my perspective, and based on the vast datasets I've been trained on, here are the key components I'd suggest you focus on:\n",
      "\n",
      "1.  **Perception Systems:** This is how an agent *observes* and *interprets* its environment. For an AI like me, my \"sensors\" are the text inputs I receive. I then use natural language processing (NLP) techniques to understand the meaning, intent, and context of your words. For physical agents, like robots, this would involve hardware sensors such as:\n",
      "    *   **Cameras:** For visual input, allowing them to \"see\" and identify objects, navigate, and recognize faces or situations.\n",
      "    *   **Microphones:** For auditory input, enabling them to hear sounds, recognize speech, and even detect anomalies like alarms.\n",
      "    *   **Lidar/Radar:** For distance sensing and mapping, crucial for navigation and obstacle avoidance in autonomous vehicles or robots.\n",
      "    *   **Tactile Sensors:** For touch and pressure, allowing them to interact physically with objects.\n",
      "    *   **GPS/IMU (Inertial Measurement Unit):** For determining location and orientation in physical space.\n",
      "\n",
      "2.  **Decision-Making/Reasoning Engine:** This is the \"brain\" of the agent. Once it has perceived its environment, it needs to decide what to do. This involves:\n",
      "    *   **Knowledge Representation:** How the agent stores and organizes information about the world. This could be through databases, ontologies, or learned representations within neural networks.\n",
      "    *   **Learning Algorithms:** Many advanced agents can learn from experience. This includes techniques like:\n",
      "        *   **Supervised Learning:** Learning from labeled examples (e.g., showing an image of a cat and labeling it \"cat\").\n",
      "        *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., clustering similar news articles).\n",
      "        *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for actions (e.g., a game-playing AI learning to win by getting positive scores).\n",
      "    *   **Planning and Search Algorithms:** For agents that need to achieve a goal, they use algorithms to figure out a sequence of actions. This is like creating a roadmap.\n",
      "    *   **Logic and Inference:** For more rule-based systems, agents might use logical rules to deduce new information or make decisions.\n",
      "\n",
      "3.  **Action/Actuation Systems:** This is how the agent *interacts* with its environment.\n",
      "    *   **For me:** My actuators are the text generation modules that produce my responses. I'm essentially \"acting\" by communicating with you.\n",
      "    *   **For physical agents:** This involves mechanisms to execute physical actions. Examples include:\n",
      "        *   **Motors and Servos:** To move robotic limbs, wheels, or other parts.\n",
      "        *   **Voice Synthesizers:** For speech output.\n",
      "        *   **Display Screens:** To show information.\n",
      "        *   **Manipulators (Grippers):** For picking up and handling objects.\n",
      "        *   **Control Systems:** To manage the execution of actions and ensure they are performed correctly and safely.\n",
      "\n",
      "4.  **Goals and Objectives:** Every agent is designed with a purpose. Understanding what the agent is trying to achieve is crucial. These goals can be simple (maintain room temperature) or incredibly complex (diagnose a disease, drive a car autonomously).\n",
      "\n",
      "5.  **Environment:** It's also important to consider the *context* in which the agent operates. Is it a simulated environment, a controlled lab setting, or the messy, unpredictable real world? The nature of the environment heavily influences the design of the perception and action systems.\n",
      "\n",
      "When I think about my own functioning, I'm constantly processing your text (perception), running through my neural network to understand the best way to respond and formulate an answer (decision-making/reasoning), and then generating this text for you (action). My overarching \"goal\" is to be helpful and informative.\n",
      "\n",
      "Does this breakdown make sense, Alex? Are there any of these components you'd like to explore in more detail? I'm happy to elaborate!\n",
      "Human: Can you remind me what my name is?\n",
      "AI: Of course, Alex! Your name is Alex. It's a pleasure to be conversing with you!\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Create memory to store conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's have a conversation!\n",
    "print(\"🗣️ Starting a conversation with memory:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First message\n",
    "response1 = conversation.predict(input=\"Hi! My name is Alex and I'm learning about AI agents.\")\n",
    "print(f\"User: Hi! My name is Alex and I'm learning about AI agents.\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "# Second message - the AI should remember the name\n",
    "response2 = conversation.predict(input=\"What are the key components I should focus on?\")\n",
    "print(f\"User: What are the key components I should focus on?\")\n",
    "print(f\"AI: {response2}\\n\")\n",
    "\n",
    "# Third message - test if it remembers the context\n",
    "response3 = conversation.predict(input=\"Can you remind me what my name is?\")\n",
    "print(f\"User: Can you remind me what my name is?\")\n",
    "print(f\"AI: {response3}\\n\")\n",
    "\n",
    "# Let's examine what's stored in memory\n",
    "print(\"🧠 Memory Contents:\")\n",
    "print(\"=\" * 30)\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cd7c9",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "For better user experience, especially with long responses, we can stream the response as it's being generated instead of waiting for the complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb3f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 Streaming Response Example:\n",
      "========================================\n",
      "Question: Explain the concept of attention mechanism in transformers in detail.\n",
      "\n",
      "Response (streaming):\n",
      "--------------------\n",
      "Let's dive deep into the concept of the **attention mechanism** in Transformers. It's the core innovation that allows these models to excel at sequential data processing, particularly in natural language processing (NLP).\n",
      "\n",
      "## The Problem Attention Solves: Limitations of Traditional RNNs/LSTMs\n",
      "\n",
      "Before attention, Recurrent Neural Networks (RNNs) and their variants like Long Short-Term Memory (LSTM) were the go-to for sequence modeling. They process information sequentially, maintaining a \"hidden state\" that summarizes past information. However, they had a few key limitations:\n",
      "\n",
      "1.  **Information Bottleneck:** As sequences get longer, the hidden state becomes a bottleneck. It has to compress all the relevant information from the entire past sequence into a fixed-size vector. This leads to forgetting information from earlier parts of the sequence.\n",
      "2.  **Difficulty with Long-Range Dependencies:** Capturing relationships between words that are far apart in a sentence (e.g., subject-verb agreement in very long sentences) was challenging.\n",
      "3.  **Lack of Parallelization:** The sequential nature of RNNs meant they couldn't be easily parallelized, making training on large datasets slow.\n",
      "\n",
      "## The Core Idea of Attention: \"Looking Back\" Selectively\n",
      "\n",
      "The attention mechanism addresses these limitations by allowing the model to **dynamically weigh the importance of different parts of the input sequence when processing each element of the output sequence.** Instead of relying solely on a compressed hidden state, attention enables the model to \"look back\" at the entire input and pick out the most relevant pieces of information.\n",
      "\n",
      "Think of it like this: when you translate a sentence, you don't just read the first word and then decide the second word of the translation. You consider the context of the entire source sentence to pick the right word. Attention mimics this human-like selective focus.\n",
      "\n",
      "## How Attention Works: The Query, Key, and Value Analogy\n",
      "\n",
      "The most common form of attention used in Transformers is **Scaled Dot-Product Attention**. It's built upon three fundamental concepts:\n",
      "\n",
      "1.  **Query (Q):** Represents the current element the model is trying to process or generate. It's like asking a question: \"What information from the input is relevant *now*?\"\n",
      "2.  **Key (K):** Represents the \"label\" or \"identifier\" of each element in the input sequence. It's like a catalog entry that describes what information that element *contains*.\n",
      "3.  **Value (V):** Represents the actual content or information associated with each element in the input sequence. It's the information that will be retrieved if the Key matches the Query.\n",
      "\n",
      "**The core process involves:**\n",
      "\n",
      "*   **Calculating Similarity:** For a given Query, the model calculates how similar it is to each Key in the input sequence.\n",
      "*   **Generating Weights (Attention Scores):** These similarity scores are then converted into probability-like weights. Higher weights indicate that the corresponding input element (represented by its Key) is more relevant to the current Query.\n",
      "*   **Weighted Sum of Values:** The final output is a weighted sum of the Values, where the weights are the attention scores. This means that the information from more relevant input elements will contribute more to the output.## The Mathematical Formulation of Scaled Dot-Product Attention\n",
      "\n",
      "Let's break down the formula:\n",
      "\n",
      "**Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V**\n",
      "\n",
      "Here's a breakdown of each part:\n",
      "\n",
      "1.  **Q * K^T (Dot Product Similarity):**\n",
      "    *   `Q` is a matrix of Query vectors.\n",
      "    *   `K` is a matrix of Key vectors.\n",
      "    *   `K^T` is the transpose of the Key matrix.\n",
      "    *   The dot product `Q * K^T` calculates the similarity between each Query vector and each Key vector. If `Q` has dimensions `(batch_size, seq_len_q, d_k)` and `K` has dimensions `(batch_size, seq_len_k, d_k)`, then `Q * K^T` will have dimensions `(batch_size, seq_len_q, seq_len_k)`. Each element `(i, j)` in this resulting matrix represents the similarity between the `i`-th Query and the `j`-th Key.\n",
      "\n",
      "2.  **/ sqrt(d_k) (Scaling):**\n",
      "    *   `d_k` is the dimension of the Key vectors.\n",
      "    *   This scaling factor is crucial. When `d_k` is large, the dot products can become very large. Large dot products can push the softmax function into regions with very small gradients, hindering learning. Scaling by the square root of `d_k` helps to stabilize the gradients.\n",
      "\n",
      "3.  **softmax(...) (Normalization):**\n",
      "    *   The `softmax` function is applied row-wise to the scaled similarity scores.\n",
      "    *   This converts the raw similarity scores into a probability distribution. For each Query, the softmax ensures that the weights for all Keys sum up to 1. These are the **attention weights**. A higher weight means that the corresponding input element is more important for the current Query.\n",
      "\n",
      "4.  **... * V (Weighted Sum of Values):**\n",
      "    *   `V` is a matrix of Value vectors.\n",
      "    *   The attention weights (the output of the softmax) are used to multiply the Value vectors.\n",
      "    *   This operation performs a weighted sum of the Value vectors. The resulting output vector for each Query is a context vector that is a blend of the input Values, with more emphasis on the Values associated with Keys that were more similar to the Query. The output dimensions will be `(batch_size, seq_len_q, d_v)`, where `d_v` is the dimension of the Value vectors.\n",
      "\n",
      "## Types of Attention in Transformers\n",
      "\n",
      "Transformers employ different types of attention mechanisms within their architecture:\n",
      "\n",
      "### 1. Self-Attention (or Intra-Attention)\n",
      "\n",
      "This is the most fundamental type of attention in Transformers. It allows the model to relate different positions of a *single* sequence to compute a representation of that sequence.\n",
      "\n",
      "*   **How it works:** In self-attention, the Queries, Keys, and Values are all derived from the *same* input sequence. This means that each word in the input sequence can attend to every other word in the same sequence (including itself).\n",
      "*   **Purpose:**\n",
      "    *   **Contextualization:** It helps the model understand the relationships between words within a sentence. For example, in \"The animal didn't cross the street because it was too tired,\" self-attention can help the model link \"it\" to \"animal.\"\n",
      "    *   **Capturing long-range dependencies:** It can directly connect words that are far apart.\n",
      "*   **In the Transformer Architecture:** Self-attention is used in both the encoder and the decoder layers.\n",
      "\n",
      "### 2. Masked Self-Attention (in the Decoder)\n",
      "\n",
      "This is a variation of self-attention used in the decoder of a Transformer.\n",
      "\n",
      "*   **How it works:** During training, the decoder generates the output sequence one token at a time. To prevent the decoder from \"cheating\" by looking at future tokens in the target sequence, a **mask** is applied. This mask sets the attention scores for future positions to negative infinity before the softmax operation, effectively making their weights zero.\n",
      "*   **Purpose:** To ensure that the prediction for a given position depends only on the outputs at previous positions, maintaining the autoregressive nature of sequence generation.\n",
      "\n",
      "### 3. Encoder-Decoder Attention (or Cross-Attention)\n",
      "\n",
      "This type of attention connects the encoder and the decoder.\n",
      "\n",
      "*   **How it works:**\n",
      "    *   **Queries (Q):** Come from the output of the decoder (specifically, the output of the masked self-attention layer in the decoder).\n",
      "    *   **Keys (K) and Values (V):** Come from the output of the encoder.\n",
      "*   **Purpose:** It allows the decoder to attend to the relevant parts of the *encoded input sequence* when generating each token of the output sequence. This is crucial for tasks like machine translation, where the decoder needs to align the output with the input. For example, when translating \"Je suis étudiant\" to \"I am a student,\" the decoder generating \"student\" will heavily attend to the encoded representation of \"étudiant.\"\n",
      "\n",
      "## Multi-Head Attention: Enhancing Attention's Power\n",
      "\n",
      "The Transformer architecture doesn't just use one attention mechanism; it uses **Multi-Head Attention**.\n",
      "\n",
      "*   **How it works:** Instead of performing a single attention function with `d_model`-dimensional Keys, Values, and Queries, Multi-Head Attention linearly projects the Queries, Keys, and Values `h` times with different learned linear projections. Then, it performs the attention function in parallel on each of these projected versions. Finally, the outputs of the `h` attention heads are concatenated and linearly projected again to produce the final output.\n",
      "*   **Purpose:**\n",
      "    *   **Learning Diverse Representations:** Each \"head\" learns to focus on different aspects or relationships within the data. For example, one head might focus on syntactic relationships, while another focuses on semantic relationships.\n",
      "    *   **Capturing Different Subspaces:** It allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "    *   **Increased\n",
      "--------------------\n",
      "✅ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Create a streaming LLM\n",
    "streaming_llm = init_chat_model(\"gemini-2.5-flash-lite\", model_provider=\"google_genai\", temperature=0.7, max_tokens=2000, streaming=True)\n",
    "\n",
    "print(\"🌊 Streaming Response Example:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Question: Explain the concept of attention mechanism in transformers in detail.\\n\")\n",
    "print(\"Response (streaming):\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Stream the response\n",
    "for chunk in streaming_llm.stream(\"Explain the concept of attention mechanism in transformers in detail.\"):\n",
    "    print(chunk.content, end='', flush=True)\n",
    "    time.sleep(0.02)  # Small delay to make streaming visible\n",
    "\n",
    "print(\"\\n\" + \"-\" * 20)\n",
    "print(\"✅ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316c666",
   "metadata": {},
   "source": [
    "#### Limits of LLM Chains - From Chains to Agents\n",
    "Chains are powerful, but they have limitations:\n",
    "- **Rigidity**: Chains follow a predetermined path. They execute steps A -> B -> C. They are not good at handling unexpected inputs or dynamically changing their course of action. This is the primary reason to use an Agent instead, which can make decisions on the fly.\n",
    "- **Error Propagation***: In a long SequentialChain, an error or a poorly-formed output from an early step will negatively impact all subsequent steps. A small \"hallucination\" in step 1 can become a major factual error by step 5.\n",
    "- **Complexity and Debugging**: Very long and complex chains can become difficult to manage, debug, and optimize. It can turn into \"prompt engineering hell,\" where tweaking one prompt breaks another one down the line.\n",
    "- **Latency and Cost**: Every step in a chain that involves an LLM is another API call. This increases the total time it takes to get a final answer and increases the cost, as you're using more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030791cd",
   "metadata": {},
   "source": [
    "Example: Interacting with APIs or External Tools\n",
    "When the LLM needs to take action or get information from the outside world (e.g., check the weather, perform a calculation, search the web).\n",
    "- This is where you move from Chains to Agents. An Agent uses an LLM not just to generate text, but to decide which \"tool\" to use next. While not strictly a \"chain,\" it's the logical next step.\n",
    "- Example: A user asks, \"What's the weather like in Paris right now, and can you write a short poem about it?\"\n",
    "The Agent's LLM decides it first needs to use the Weather API tool.\n",
    "It calls the tool with \"Paris\" as the input and gets the current weather data.\n",
    "It then uses that data as context for a second LLM call to generate the poem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ce33b",
   "metadata": {},
   "source": [
    "## How are LLMs used in AI Agents?\n",
    "\n",
    "LLMs are the core intelligence of AI Agents, enabling them to comprehend and produce human language. They are responsible for interpreting user instructions, maintaining conversational context, formulating plans, and selecting appropriate tools. For now, it's essential to understand that the LLM serves as the agent's \"brain,\" a concept we will explore in greater detail later in this Unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fffb6",
   "metadata": {},
   "source": [
    "### Practical Exercise: Building a Simple AI Assistant using a LLMChain\n",
    "\n",
    "Let's combine everything we've learned to build a simple AI assistant that can help with different tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b94b30cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI Assistant Created!\n",
      "You can now use:\n",
      "- assistant.chat('your message')\n",
      "- assistant.explain('topic')\n",
      "- assistant.help_with_code('coding question')\n",
      "- assistant.clear_memory()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/_r3hvp4d0qx6w_ny6brbf6nw0000gn/T/ipykernel_23577/695309948.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferWindowMemory(k=5)  # Keep last 5 exchanges\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "class SimpleAIAssistant:\n",
    "    \"\"\"A simple AI assistant that can help with various tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        # Use window memory to keep only recent conversation history\n",
    "        self.memory = ConversationBufferWindowMemory(k=5)  # Keep last 5 exchanges\n",
    "        \n",
    "        # Define different prompt templates for different tasks\n",
    "        self.templates = {\n",
    "            'general': \"\"\"You are a helpful AI assistant. You are knowledgeable, friendly, and concise.\n",
    "            \n",
    "            Previous conversation:\n",
    "            {history}\n",
    "            \n",
    "            Human: {input}\n",
    "            Assistant:\"\"\",\n",
    "            \n",
    "            'explain': \"\"\"You are an expert educator. Explain complex topics in simple, easy-to-understand terms.\n",
    "            \n",
    "            Previous conversation:\n",
    "            {history}\n",
    "            \n",
    "            Topic to explain: {input}\n",
    "            \n",
    "            Explanation:\"\"\",\n",
    "            \n",
    "            'code': \"\"\"You are a coding assistant. Help with programming questions and provide clean, well-commented code.\n",
    "            \n",
    "            Previous conversation:\n",
    "            {history}\n",
    "            \n",
    "            Coding request: {input}\n",
    "            \n",
    "            Response:\"\"\"\n",
    "        }\n",
    "        \n",
    "    def chat(self, message, task_type='general'):\n",
    "        \"\"\"Chat with the assistant.\"\"\"\n",
    "        template = self.templates.get(task_type, self.templates['general'])\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=['history', 'input'],\n",
    "            template=template\n",
    "        )\n",
    "        \n",
    "        chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=prompt,\n",
    "            memory=self.memory\n",
    "        )\n",
    "        \n",
    "        response = chain.predict(input=message)\n",
    "        return response\n",
    "    \n",
    "    def explain(self, topic):\n",
    "        \"\"\"Ask the assistant to explain a topic.\"\"\"\n",
    "        return self.chat(topic, task_type='explain')\n",
    "    \n",
    "    def help_with_code(self, coding_request):\n",
    "        \"\"\"Ask for coding help.\"\"\"\n",
    "        return self.chat(coding_request, task_type='code')\n",
    "    \n",
    "    def print_memory(self):\n",
    "        \"\"\"Print the conversation history.\"\"\"\n",
    "        print(\"🧠 Conversation History:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(self.memory.buffer)\n",
    "        print(\"=\" * 30)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear the conversation memory.\"\"\"\n",
    "        self.memory.clear()\n",
    "        print(\"🧠 Memory cleared!\")\n",
    "\n",
    "# Create our AI assistant\n",
    "assistant = SimpleAIAssistant(llm)\n",
    "\n",
    "print(\"🤖 AI Assistant Created!\")\n",
    "print(\"You can now use:\")\n",
    "print(\"- assistant.chat('your message')\")\n",
    "print(\"- assistant.explain('topic')\")\n",
    "print(\"- assistant.help_with_code('coding question')\")\n",
    "print(\"- assistant.clear_memory()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e7747c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'd be happy to help you with your agentic AI course. What can I assist you with today? Do you have specific questions about concepts, algorithms, or perhaps some assignments?\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.chat('Hi please help me with my course on agentic artificial intelligence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7303f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, imagine you have a super-smart toy robot, right?\\n\\nThis robot isn\\'t just like a regular toy that only does what you tell it to do when you press a button. This robot is special because it can **think a little bit for itself** and **decide what to do** to achieve a goal.\\n\\nThink about it like this:\\n\\n*   **Your regular toy robot:** If you press the \"walk forward\" button, it walks forward. If you press \"turn left,\" it turns left. It only does *exactly* what you tell it.\\n\\n*   **An \"agentic\" robot (or AI):** This robot might have a goal, like \"clean up the toys.\" Instead of you telling it \"pick up this block,\" then \"put it in the box,\" then \"pick up that car,\" then \"put it in the box,\" the agentic robot can **figure out how to do it on its own.**\\n\\nIt might:\\n\\n1.  **See** the toys.\\n2.  **Decide** which toy to pick up first (maybe the closest one).\\n3.  **Move** to the toy.\\n4.  **Pick up** the toy.\\n5.  **Find** the toy box.\\n6.  **Put** the toy in the box.\\n7.  Then, it **looks around again** to see if there are any more toys left, and it **starts all over** to pick up the next one!\\n\\nSo, an \"agentic\" AI is like a smart helper that can:\\n\\n*   **See** things (like the toys).\\n*   **Think** about what it needs to do to reach a goal (like cleaning up).\\n*   **Act** on its own to do it.\\n\\nIt\\'s like giving a toy a little brain so it can be more helpful and do things without you having to tell it every single tiny step. Does that make sense?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.explain('I dont know how to explain this to my 5 year old.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b74df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Conversation History:\n",
      "==============================\n",
      "Human: Hi please help me with my course on agentic artificial intelligence.\n",
      "AI: Hi there! I'd be happy to help you with your agentic AI course. What can I assist you with today? Do you have specific questions about concepts, algorithms, or perhaps some assignments?\n",
      "Human: I dont know how to explain this to my 5 year old.\n",
      "AI: Okay, imagine you have a super-smart toy robot, right?\n",
      "\n",
      "This robot isn't just like a regular toy that only does what you tell it to do when you press a button. This robot is special because it can **think a little bit for itself** and **decide what to do** to achieve a goal.\n",
      "\n",
      "Think about it like this:\n",
      "\n",
      "*   **Your regular toy robot:** If you press the \"walk forward\" button, it walks forward. If you press \"turn left,\" it turns left. It only does *exactly* what you tell it.\n",
      "\n",
      "*   **An \"agentic\" robot (or AI):** This robot might have a goal, like \"clean up the toys.\" Instead of you telling it \"pick up this block,\" then \"put it in the box,\" then \"pick up that car,\" then \"put it in the box,\" the agentic robot can **figure out how to do it on its own.**\n",
      "\n",
      "It might:\n",
      "\n",
      "1.  **See** the toys.\n",
      "2.  **Decide** which toy to pick up first (maybe the closest one).\n",
      "3.  **Move** to the toy.\n",
      "4.  **Pick up** the toy.\n",
      "5.  **Find** the toy box.\n",
      "6.  **Put** the toy in the box.\n",
      "7.  Then, it **looks around again** to see if there are any more toys left, and it **starts all over** to pick up the next one!\n",
      "\n",
      "So, an \"agentic\" AI is like a smart helper that can:\n",
      "\n",
      "*   **See** things (like the toys).\n",
      "*   **Think** about what it needs to do to reach a goal (like cleaning up).\n",
      "*   **Act** on its own to do it.\n",
      "\n",
      "It's like giving a toy a little brain so it can be more helpful and do things without you having to tell it every single tiny step. Does that make sense?\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "assistant.print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7f410bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Memory cleared!\n"
     ]
    }
   ],
   "source": [
    "assistant.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3d0b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Conversation History:\n",
      "==============================\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "assistant.print_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_AAI",
   "language": "python",
   "name": "agentic_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
